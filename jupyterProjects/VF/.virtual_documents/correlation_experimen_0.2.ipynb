


import pandas as pd
import numpy as np








from util.data_loader import data_loader2
data_name='bank_data_B'
miss_rate=0.2
complete_data_B,incomplete_data_B=data_loader2(data_name,miss_rate)


print(complete_data_B.shape)
print(incomplete_data_B.shape)








discrete_columns=incomplete_data_B.select_dtypes(include=['object'])


incomplete_data_B.info()


discrete_columns


from ctgan import CTGAN
ctgan = CTGAN(epochs=10)
ctgan.fit(incomplete_data_B,discrete_columns)


len1=complete_data_B.shape[0]
len2=incomplete_data_B.shape[0]
print(len1)
print(len2)


# Create synthetic data
# synthetic_data = ctgan.sample(len1-len2)
# 生成数据
synthetic_data = ctgan.sample(len1)


print(synthetic_data.shape)
synthetic_data.head(10)


synthetic_data.isna().sum()


print(incomplete_data_B.shape)
incomplete_data_B.head(50)





correlation_columns_to_keep=['previous','pdays','poutcome']
# 创建一个新的DataFrame，保留要保留的列，其他列设为NaN
correlation_synthetic_data = synthetic_data.copy()

# correlation_synthetic_data=pd.DataFrame(correlation_synthetic_data)
correlation_synthetic_data.shape


for column in correlation_synthetic_data.columns:
    if column not in correlation_columns_to_keep:
        correlation_synthetic_data[column] = np.nan


print(correlation_synthetic_data.shape)
correlation_synthetic_data.head(10)





complete_data_A = pd.read_csv('datasetsAB/bank_data_A.csv', sep=',')
complete_data_A.shape


incomplete_data_B.shape


# 假设A和B是你的DataFrame，你想保留A中的重复列
aligned_df = complete_data_A.merge(incomplete_data_B, left_index=True, right_index=True, how='outer', suffixes=('_keep', '_drop'))

# 删除B中的重复列
columns_to_drop = [column for column in aligned_df.columns if '_drop' in column]
aligned_df = aligned_df.drop(columns_to_drop, axis=1)

# 删除列名中的后缀
aligned_df.columns = aligned_df.columns.str.replace('_keep', '')


print(aligned_df.shape)
aligned_df.head()


# 假设df1和df2是你的两个DataFrame
aligned_df_new = aligned_df.combine_first(correlation_synthetic_data)


aligned_df_new=aligned_df_new.drop('y',axis=1)
print(aligned_df_new.shape)
aligned_df_new.head()


aligned_df.shape[0] - incomplete_data_B.shape[0]


aligned_df_new.columns[aligned_df_new.isna().any()]


len(incomplete_data_B.columns)





aligned_df_new_encode=aligned_df_new.copy()
aligned_df_new_encode.head()






from sklearn.preprocessing import LabelEncoder
# 创建一个标签编码器
le = LabelEncoder()
# 对每一列进行检查
for col in aligned_df_new_encode.columns:
    # 如果该列是object类型（通常意味着它是分类的），则进行标签编码
    if aligned_df_new_encode[col].dtype == 'object':
        # 若该列有NaN值，将NaN值转换为“NaN”字符串
        # if aligned_df_new_encode[col].isnull().any():
        #    aligned_df_new_encode[col]=aligned_df_new_encode.fillna('NaN')
         # 保存原始的NaN值位置
        nan_idx=aligned_df_new_encode[col].isnull()
        # 进行特征编码，并不对nan值进行编码
        aligned_df_new_encode.loc[~nan_idx,col] =le.fit_transform(aligned_df_new_encode.loc[~nan_idx,col])
        #aligned_df_new_encode[col] =le.fit_transform(aligned_df_new_encode[col])
        # 将“NaN”字符串对应的编码再次转换回NaN
        # aligned_df_new_encode[col]=aligned_df_new_encode[col].replace(le.transform(['NaN'])[0], np.nan)



print(aligned_df_new_encode.shape)
aligned_df_new_encode.head()


print(aligned_df_new_encode.dtypes)


# 使用  填充 NaN
aligned_df_new_encode= aligned_df_new_encode.fillna(-9999.999)
print(aligned_df_new_encode.dtypes)


aligned_df_new_encode[aligned_df_new_encode == -9999.999] = np.nan
print(aligned_df_new_encode.dtypes)


# aligned_df_new_encode.to_csv('datasetsAB/aligned_df_new_encode.csv', index=False)





# 假设bank_data是原始数据集
bank_data = pd.read_csv('datasets/bank.csv',sep=';')
bank_data['index']=range(0,len(bank_data))
bank_data=bank_data.drop('y',axis=1)
print(bank_data.shape)
bank_data.head()


bank_data_encode=bank_data.copy()
# 对每一列进行检查
for col in bank_data_encode.columns:
    # 如果该列是object类型（通常意味着它是分类的），则进行标签编码
    if bank_data_encode[col].dtype == 'object':

        bank_data_encode[col] =le.fit_transform(bank_data_encode[col])
bank_data_encode.head()


aligned_df_new_encode.head()


# 将bank_data_encode数据集的列的顺序调整到和aligned_df_new_encode列的顺序一样
bank_data_encode = bank_data_encode.reindex(columns=aligned_df_new_encode.columns)
bank_data_encode.head()





data_m = 1 - np.isnan(aligned_df_new_encode.values)  # 创建掩模矩阵，标记缺失值位置
data_m





gain_parameters = {'batch_size': 64,
                   'hint_rate': 0.9,
                   'alpha': 100,
                   'iterations': 10000}
from method.gain import gain
# 填补缺失数据
# 这里传入的不完整数据集需要是numarry类型，不能是DataForm类型
imputed_data_x = gain(aligned_df_new_encode.values, gain_parameters)
# 返回的imputed_data_x是num类型


pd.DataFrame(imputed_data_x)


# 查看imputed_data_x的类型
print(pd.DataFrame(imputed_data_x).dtypes)


# 将原始数据集也改为和填补数据集一样的类型
bank_data_encode=bank_data_encode.astype(float)
print(bank_data_encode.dtypes)


bank_data_encode





from VF.util.utils import rmse_loss
# 计算均方误差
rmse = rmse_loss(bank_data_encode.values, imputed_data_x, data_m)
rmse_spam=str(np.round(rmse, 4))
print('RMSE（均方误差）为: ' + rmse_spam)


imputed_data_x=pd.DataFrame(imputed_data_x)


# 假设df1和df2是你的两个数据集，你想让df2的列名和df1的一样
# 首先，获取df1的列名
columns_df1 = bank_data_encode.columns

# 然后，获取df2的列名
columns_df2 = imputed_data_x.columns

# 创建一个字典，其中键是df2的列名，值是df1的列名
rename_dict = {old: new for old, new in zip(columns_df2, columns_df1)}

# 使用rename函数修改df2的列名
imputed_data_x.rename(columns=rename_dict, inplace=True)


imputed_data_x.head(10)


# 获得目标列，假设列名是'column_name'
column = pd.read_csv('datasets/bank.csv',sep=';')['y']
# 将这一列添加到其他数据集中，假设你想把这一列的名字还叫'column_name'
bank_data_encode['y'] = column
imputed_data_x['y'] = column
print(bank_data_encode.shape)
print(imputed_data_x.shape)


from table_evaluator import TableEvaluator
# 传入是数据类型得是DataForm类型
evaluator=TableEvaluator(bank_data_encode,imputed_data_x)
# 数值分析
evaluator.evaluate(target_col='y',notebook=True,verbose=False)


evaluator.evaluate(target_col='y',notebook=False,verbose=False)


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report
from sklearn.metrics import  confusion_matrix
X = imputed_data_x.drop('y', axis=1)  # 假设 '' 是目标变量
# 将样本集划分为70%的训练集，30%作为测试集
X_train, X_test, y_train, y_test = train_test_split(X, column, test_size=0.3, random_state=42)
# 创建和训练模型
model = LogisticRegression()
model.fit(X_train, y_train)
# 预测
y_pred = model.predict(X_test)
# 计算准确率
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy: ', accuracy)
#测试逻辑回归的模型评估

y_test = le.fit_transform(y_test)
y_pred = le.fit_transform(y_pred)
auc=roc_auc_score(y_test, y_pred)
print('auc: ', auc)



