# PU学习的一些技术方法

PU（Positive-Unlabeled）学习问题近年来受到广泛关注。由于标准的机器学习问题（即同时使用大量正例和负例数据点来训练模型）已经有许多成熟的解决方法，大多数PU学习方法是对这些标准方法的巧妙改进。

以下是几种常见PU学习技术的总结，其中大部分会在实验中应用。

---

## 1. **直接应用标准分类器**

最简单的PU学习方法是：将正例数据点视为正例，将未标记数据点视为负例，直接训练一个标准分类器。分类器会为每个数据点分配一个分数，正例通常会得到较高的分数。在未标记数据点中（暂时被标记为负例），分数最高的那些点最有可能是正例。

这一“最朴素”的方法在Elkan和Noto的论文[*Learning classifiers from only positive and unlabeled data*](http://users.csc.tntech.edu/~weberle/Fall2008/CSC6910/Papers/posonly.pdf)（2008）中有所探讨并得到一定的理论支持。论文的核心结论是：在某些基本但可能不完全符合实际的假设下，用正例和未标记数据训练的分类器，其分数与用正例和负例数据训练的分类器的分数成比例。

正如作者所说：“这意味着，如果[假设正确训练的]分类器仅用于根据属于正例的概率对样本进行排序，那么[用正例和未标记数据训练的]分类器可以直接使用。”

---

## 2. **PU Bagging**

一种更复杂的PU学习方法是对传统[Bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating)技术的改进：

1. 将所有正例数据点与未标记数据点的随机样本（有放回抽样）组合成一个训练集。
2. 将正例和未标记数据点分别视为正例和负例，基于该训练集训练一个分类器。
3. 将分类器应用于未包含在随机样本中的未标记数据点（称为OOB，即“袋外”数据点），记录它们的分数。
4. 重复上述三步多次，最终为每个数据点分配其OOB分数的平均值。

这一方法在Mordelet和Vert的论文[*A bagging SVM to learn from positive and unlabeled examples*](http://members.cbio.mines-paristech.fr/~jvert/svn/bibli/local/Mordelet2013bagging.pdf)（2013）中有所描述。作者指出：“该方法的性能可以匹敌甚至超越最先进的PU学习方法，尤其是在正例样本数量有限且未标记样本中负例比例较小时。此外，当未标记样本集较大时，该方法的运行速度显著快于其他方法。”

---

## 3. **两步法**

许多PU学习策略属于“两步法”的范畴。[*An Evaluation of Two-Step Techniques for Positive-Unlabeled Learning in Text Classification*](http://ijcat.com/archives/volume3/issue9/ijcatr03091012.pdf)（2014）由Kaboutari、Bagherzadeh和Kheradmand撰写的论文对这种方法进行了介绍。

“两步法”的核心步骤如下：

1. 确定未标记数据点中可以被自信地标记为负例的子集（称为“可靠负例”）。
2. 使用正例和负例数据点训练一个标准分类器，并将其应用于剩余的未标记数据点。

通常，第二步的结果会被用来返回第一步并进行迭代。正如上述论文的作者所述：“如果[可靠负例]集合主要包含负例且足够大，学习算法可以很好地工作并构建一个优秀的分类器。但如果第一步仅识别出一个非常小的负例集合，学习算法会迭代运行，直到收敛或满足某个停止条件。”

类似的方法也在Shubham Jain的博客[*Introduction to Pseudo-Labelling: A Semi-Supervised learning technique*](https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/)中有所提及，但该博客的细节并未专门针对PU问题。





### **1. 当前三种方法存在的问题**

#### **(1) 直接应用标准分类器**
- **问题：假设不合理**
  - 该方法假设未标记数据点中的负例占比较大（即未标记数据点主要是负例），但在实际场景中，这一假设可能并不成立。如果未标记数据中正例比例较高，分类器的性能会显著下降。
  - 分类器的分数仅能用于排序，无法直接反映样本属于正例的真实概率，可能导致分类边界不准确。
- **问题：对未标记数据的利用不足**
  - 该方法没有充分挖掘未标记数据中的潜在信息，仅将其简单视为负例，忽略了未标记数据中可能存在的正例。

#### **(2) PU Bagging**
- **问题：对随机性依赖较大**
  - 每次随机抽样未标记数据点时，可能会引入噪声。如果随机样本中包含较多正例，会导致分类器的性能下降。
- **问题：未充分利用可靠负例**
  - PU Bagging没有明确区分未标记数据中的可靠负例，而是将所有未标记数据点视为潜在负例，可能导致分类器的误差较大。
- **问题：计算成本较高**
  - 由于需要多次随机抽样和训练分类器，PU Bagging在大规模数据集上的计算成本较高。

#### **(3) 两步法**
- **问题：可靠负例的选择困难**
  - 在第一步中，如何准确地选择可靠负例是一个关键问题。如果选择的可靠负例中包含较多正例，会严重影响后续分类器的性能。
- **问题：对初始选择依赖较大**
  - 两步法对初始可靠负例的选择依赖性较强，初始选择的质量会直接影响后续迭代的效果。如果初始选择不准确，可能导致分类器陷入局部最优。
- **问题：可能需要多次迭代**
  - 两步法通常需要多次迭代才能收敛，迭代过程可能会引入累积误差，导致分类器性能下降。

---

### **2. PU Bagging + 两步法结合方法的优点**

#### **方法描述**
- **外层循环**：迭代选择正样本和负样本。
  - 在每次迭代中，通过两步法的思想，动态调整可靠负例和正例的选择。
- **内层循环**：在每次迭代中，使用PU Bagging对样本进行评分。
  - 通过PU Bagging的随机采样和OOB评分机制，为每个样本分配一个更加稳健的分数。

#### **优点**
1. **充分利用未标记数据的信息**
   - 结合两步法的可靠负例选择机制，能够更准确地从未标记数据中挖掘负例信息。
   - PU Bagging通过多次随机采样和评分，能够更全面地利用未标记数据中的潜在信息。

2. **提高分类器的鲁棒性**
   - PU Bagging的随机采样机制可以降低单次采样的噪声影响，通过多次采样和OOB评分，能够得到更加稳健的样本分数。
   - 两步法的迭代机制可以动态调整正例和负例的选择，逐步优化分类器的性能。

3. **缓解初始选择的依赖性**
   - 两步法中的可靠负例选择对初始选择依赖较大，而PU Bagging的随机采样机制可以在一定程度上缓解这一问题。即使初始选择不够准确，PU Bagging的多次采样和评分机制可以弥补初始选择的不足。

4. **更高的分类精度**
   - 外层循环通过两步法不断优化正例和负例的选择，内层循环通过PU Bagging为每个样本分配更加准确的分数，二者结合能够显著提高分类器的精度。

5. **降低误差累积的风险**
   - 两步法的多次迭代可能会引入累积误差，而PU Bagging的评分机制可以在每次迭代中为样本提供更加稳健的分数，从而降低误差累积的风险。

6. **适应不同的正负样本比例**
   - 结合两步法和PU Bagging的优点，该方法对未标记数据中正负样本比例的依赖性较低，能够适应正例比例较高或较低的情况。

7. **适合大规模数据集**
   - 虽然PU Bagging的计算成本较高，但通过两步法的可靠负例选择，可以减少内层循环的计算量，从而提高方法在大规模数据集上的效率。

---

### **总结**
PU Bagging + 两步法结合的方法通过外层的迭代优化和内层的稳健评分机制，能够有效克服当前三种方法的缺点。它既能充分利用未标记数据的信息，又能提高分类器的鲁棒性和精度，同时适应不同的数据分布和规模，是一种更加全面和高效的PU学习方法。



# 优点

- 充分挖掘了负样本的信息
- 降低了对随机性的依赖
- 更高的分类精度