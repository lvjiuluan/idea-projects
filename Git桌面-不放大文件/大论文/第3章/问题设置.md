### General Scenario: K Data Owners and a Central Server

In the context of this discussion, we begin by exploring a distributed data scenario involving K distinct data owners, each of whom maintains control over a unique portion of a larger dataset, alongside a single central server that serves as a coordinator for their collaborative efforts. The data possessed by each of these K data owners is systematically organized into a matrix, specifically denoted as ${{\mathsf{\mathcal{D}}}_{k}}$, where the subscript k identifies the k-th data owner in this group. Within this matrix, each row corresponds to an individual sample, representing a distinct data instance or observation, while each column corresponds to a specific feature, capturing a measurable attribute or characteristic associated with those samples. Collectively, the entire dataset across all parties can be abstractly represented as a structured triplet, $(\mathsf{\mathcal{I}}, \mathsf{\mathcal{X}}, \mathsf{\mathcal{Y}})$, where each component carries a precise meaning: $\mathsf{\mathcal{I}}$ signifies the sample ID space, encompassing the unique identifiers assigned to each sample; $\mathsf{\mathcal{X}}$ denotes the feature space, which encapsulates the full range of possible feature values across all samples; and $\mathsf{\mathcal{Y}}$ represents the label space, which includes all possible classification labels or categories that can be assigned to the samples. In the traditional framework of vertical federated learning (VFL), a key assumption is typically made: at least one of the participating parties possesses a complete set of labels for its respective portion of the dataset, enabling the training of predictive models with full supervision. However, this assumption often does not hold in numerous real-world situations, where obtaining fully labeled datasets becomes a formidable challenge, largely due to practical business constraints such as privacy concerns, regulatory restrictions, high annotation costs, or competitive sensitivities that limit data sharing and labeling efforts.

---

### Illustrative Example: Collaboration Among Three Data Owners

To provide a clearer and more tangible understanding of this concept, let us now consider a specific illustrative scenario involving three distinct data owners, whom we designate as party A, party B, and party C. Each of these parties holds sensitive data—information that is critical to their operations but which they are reluctant to disclose outright due to privacy and security concerns. As a result, they must collaborate in a secure manner, leveraging their collective data resources while ensuring that individual data privacy is meticulously preserved. Within this scenario, the data samples under consideration can be broadly categorized into two fundamental classes: positive and negative, representing, for instance, desirable versus undesirable outcomes or behaviors in a given application context. A notable characteristic of this setup is that all three parties share some degree of overlap in their sample identifiers (IDs), meaning that certain samples appear in the datasets of multiple parties, though the associated features may differ depending on what each party collects or observes.

More specifically, party A possesses a collection of samples denoted as $P$, which is unique in that it exclusively contains positive samples—data instances that are definitively labeled as belonging to the positive class, with no negative or ambiguous cases included. In contrast, parties B and C collectively hold a set of unlabeled data, which we denote as $U$, encompassing samples whose class labels (positive or negative) remain unknown. Importantly, $U$ is defined to exclude any samples already present in party A’s set $P$, ensuring that it represents a distinct pool of unlabeled data separate from the positive samples owned by party A. The primary objective of these three parties is to jointly develop a recommendation model through their collaboration. This model is designed to analyze the unlabeled dataset $U$ and identify samples that can be reliably classified as positive, despite the absence of explicit labels for these samples. We denote the output of this process as $R$, which represents the collection of reliable positive samples that the model successfully extracts from $U$. Once identified, $R$ can be securely provided to party A, enabling them to deliver targeted product recommendations to their customers or users based on these inferred positive instances, thereby enhancing their business offerings.

---

### Challenges with Conventional Vertical Federated Learning

This problem setting introduces a significant complication that renders conventional vertical federated learning (VFL) algorithms inapplicable in their standard form. The core issue lies in the fact that none of the participating clients—party A, party B, or party C—possesses a complete set of labels for their respective datasets. In traditional VFL, the presence of at least one party with fully labeled data is a foundational requirement, as it allows the model to learn from a supervised signal that can be propagated across the federated system. However, in this case, party A has only positive labels (for $P$), while parties B and C have no labels at all (for $U$), creating a gap that prevents the direct use of standard VFL techniques.

To address this challenge, one might consider adopting semi-supervised learning strategies, which are designed to handle datasets that include both labeled and unlabeled instances. A particularly relevant approach in this context is PU learning (Positive and Unlabeled learning), a specialized semi-supervised technique that trains models using a set of positively labeled samples ($P$) alongside a set of unlabeled samples ($U$), with the goal of distinguishing positive instances within the unlabeled set. PU learning is well-suited to scenarios where negative labels are absent or unreliable, making it a promising candidate for this problem. However, a critical limitation arises: traditional PU learning methods assume that the learner has simultaneous access to both $P$ and $U$ during the training process, allowing the algorithm to directly compare and contrast the positive and unlabeled data. In the federated scenario described here, this assumption does not hold, as party A only possesses $P$, while $U$ is distributed across parties B and C. Due to privacy constraints, these parties cannot simply pool their data into a single repository or share it freely with party A, introducing a significant barrier to applying PU learning in its conventional form.

---

### The Unlabeled-Data-Deficient PU (UDD-PU) Learning Problem

As a consequence of these circumstances, party A, which seeks to integrate recommendation services into its business operations, faces a novel and complex challenge that we designate as the Unlabeled-Data-Deficient PU (UDD-PU) learning recommendation problem. This problem is characterized by the fact that party A has access to only the positive samples in $P$ and lacks direct access to the unlabeled data $U$ necessary to perform traditional PU learning. Furthermore, the federated nature of the data distribution—with $U$ split between parties B and C—and the stringent privacy requirements imposed by all parties mean that existing PU learning methodologies cannot be directly applied without significant adaptation. The UDD-PU problem thus represents a unique intersection of federated learning, semi-supervised learning, and privacy-preserving computation, requiring innovative solutions to enable effective collaboration and model training under these constraints.

In summary, this expanded explanation highlights the intricacies of the scenario, the limitations of traditional approaches, and the emergence of a new problem formulation, providing a detailed foundation for further exploration or methodological development in a research paper.

# 中文

在本讨论的语境中，首先探讨一个分布式数据场景：K个独立的数据所有者各自掌控着大型数据集的不同部分，并存在一个中央服务器作为协作协调者。每个数据所有者持有的数据被系统地组织成矩阵形式，具体记作${{\mathsf{\mathcal{D}}}_{k}}$（下标k表示第k个数据所有者）。该矩阵中，每行对应一个独立样本（即具体的数据实例或观测值），每列对应特定特征（即样本的可测量属性或特性）。从全局视角，整个分布式数据集可抽象为结构化三元组$(\mathsf{\mathcal{I}}, \mathsf{\mathcal{X}}, \mathsf{\mathcal{Y}})$，其中：$\mathsf{\mathcal{I}}$为样本ID空间，包含所有样本的唯一标识符；$\mathsf{\mathcal{X}}$为特征空间，涵盖所有样本的全部特征维度；$\mathsf{\mathcal{Y}}$为标签空间：包含所有可能的分类标签或类别。

在传统纵向联邦学习（Vertical Federated Learning, VFL）框架中，通常存在一个关键假设：至少有一个参与方持有其数据部分的完整标签集，从而能够实现有监督的预测模型训练。然而，这一假设在大量现实场景中往往难以成立，因为获取全量标注数据集面临严峻挑战。这种挑战主要源于实际商业环境中的多重约束，包括但不限于：隐私保护法规对数据使用的限制、高昂的人工标注成本、行业竞争导致的数据孤岛现象、敏感信息共享的法律风险以及动态数据更新带来的标注滞后问题。

为了更清晰、更具体地理解这一概念，现在考虑一个具体的示例场景，其中涉及三位不同的数据拥有者，将其分别称为 A 方、B 方和 C 方。这三方各自持有敏感数据——这些信息对其业务至关重要，但由于隐私和安全方面的考虑，他们不愿直接公开。因此，他们需要在确保数据隐私严格保护的前提下，以安全的方式进行合作，充分利用各自的数据资源。

在该场景中，所涉及的数据样本可以被划分为两大基本类别：正类（positive）和负类（negative），这可以对应于某些应用场景中的理想（或期望）行为与非理想（或不期望）行为。一个值得注意的特点是，这三方在样本标识符（ID）上存在一定程度的重叠，即某些样本可能同时出现在多个参与方的数据集中，但每一方所收集或观察到的特征可能有所不同。

具体而言，A 方持有一个样本集合，记作 $ P $，该集合的独特之处在于它仅包含正类样本——即被明确标记为正类的数据实例，不包含任何负类或不确定的样本。而 B 方和 C 方共同持有一个未标注数据集，记作 $ U $，其中的样本类别（正类或负类）未知。需要特别强调的是，$ U $ 不包含 A 方数据集 $ P $ 中的任何样本，即 $ U $ 是一个独立的数据集，不与 A 方的正类样本重叠。

这三方的主要目标是通过合作共同训练一个推荐模型，该模型的作用是分析未标注数据集 $ U $，并从中识别出可以被可靠归类为正类的样本。这一过程的输出结果记作 $ R $，即从 $ U $ 中成功提取出的可靠正类样本集合。一旦这些样本被识别出来，$ R $ 将被提供给 A 方，使其能够基于这些推断出的正类实例向客户或用户提供精准的产品推荐，从而提升其业务能力和市场竞争力。

该问题设置引入了一个重大挑战，使得传统的纵向联邦学习（VFL）算法在其标准形式下无法适用。核心问题在于，参与的各方——A 方、B 方和 C 方——都不拥有完整的标签数据。在传统的 VFL 方案中，至少需要有一方持有完整的标注数据，以便模型能够从监督信号中学习，并在联邦系统中传播。然而，在本问题中，A 方仅拥有正类样本的标签（即数据集 $ P $），而 B 方和 C 方的数据集 $ U $ 完全没有标签，这导致了一个关键的缺口，使得标准的 VFL 技术无法直接应用。

为了解决这一挑战，可以考虑采用半监督学习（semi-supervised learning）策略，该策略专门用于处理包含部分标注数据和未标注数据的场景。在本问题背景下，一个特别相关的方法是PU 学习（Positive and Unlabeled learning，正类与未标注学习）。PU 学习是一种特殊的半监督学习技术，它利用一组带有正类标签的样本（$ P $）和一组未标注样本（$ U $）进行训练，目标是在未标注数据集中识别出正类实例。PU 学习特别适用于缺少负类标签或负类标签不可靠的情况，因此在本问题中具有较大的应用潜力。

然而，PU 学习在本场景下的直接应用存在一个关键限制：传统的 PU 学习方法假设学习者可以同时访问 $ P $ 和 $ U $，从而能够直接对比正类样本和未标注样本，并进行有效的训练。然而，在本联邦学习场景中，这一假设并不成立，因为 A 方仅持有 $ P $，而 $ U $ 分布在 B 方和 C 方手中。由于隐私保护的要求，各方无法简单地将数据汇总到一个集中存储库，也不能自由共享数据给 A 方，这使得传统 PU 学习方法难以直接应用，成为本问题的一个重要挑战。

在这种情况下，A 方希望将推荐服务集成到其业务运营中，但面临一个全新的复杂挑战，将其称为“未标注数据缺失的 PU 学习推荐问题”（Unlabeled-Data-Deficient PU, UDD-PU）。该问题的核心特征在于，A 方仅能访问正类样本集 $ P $，但无法直接获取未标注数据 $ U $，这使得传统的 PU 学习方法无法直接应用。此外，由于数据在联邦学习环境下分布——即 $ U $ 分散存储在 B 方和 C 方手中——并且所有参与方都施加了严格的隐私保护要求，因此现有的 PU 学习方法若不进行重大改进，无法直接适用于该场景。

因此，UDD-PU 问题代表了联邦学习、半监督学习和隐私保护计算的独特交汇点，需要创新性的解决方案，以便在这些约束条件下实现有效的协作和模型训练。

总的来说，这一扩展解释突出了该场景的复杂性、传统方法的局限性，以及新问题的提出，为进一步的研究探索或方法论发展奠定了详细的基础，可用于撰写相关研究论文。
