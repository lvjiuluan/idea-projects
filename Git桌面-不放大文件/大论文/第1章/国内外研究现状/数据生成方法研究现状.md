```
目前表现比较优秀的数据生成方法主要有基于自编码的生成模型、生成对抗网络（Generative Adversarial Networks, GAN）、扩散模型（Diffusion Probabilistic Models，DDPM）等，旨在通过已有的样本来生成新的样本数据。
基于自编码的生成模型，如：自编码器AE[35]、变分自编码器VAE[37]等。L. Xu [38]等人提出的是对变分自编码器（VAE）的改进方法，通过对隐变量与表格数据特征的联合分布进行专门建模，以实现对连续和离散特征的有效生成与重构。
GAN[39]是 I. J. Goodfellow 等人在2014年提出的模型，GAN受博弈论的启发，内部有生成器（G）和判别器（D）两个网络，两个网络相互对抗博弈达到有效生成数据的目的。M. Mirza 等人[40]提出的CGANs（Conditional Generative Adversarial Nets）在生成模型和判别模型的建模中均引入条件变量，即数据的标签，将GANs从无监督学习变成有监督学习。M. Arjovsky 等人[42, 43]引入Wasserstein距离来替代JS散度和KL散度，并将其作为优化目标，从而提出了WGAN（Wasserstein GAN），从根本上解决了原始GAN的梯度消失问题。L. Xu 等人[44]提出了一种生成对抗网络Tabular GAN (TGAN)，使用LSTM和MLP分别作为生成器和判别器，生成如病例或教育记录等表格数据。2019年，L. Xu 等人[38]提出了一种基于Conditional GAN的CTGAN来生成表格数据，用于对表格数据分布和样本行进行建模。Y. Yu 等人[45]提出了一种基于改进的条件生成对抗网络CWGAN(Conditional Wasserstein Generative Adversarial Nets)来学习滚动轴承故障的时频谱特征，并根据输入类别生成相应故障类别的时频谱。J. Lee 等人[47]提出了一个广义的GAN框架的表格合成，它结合了GAN的对抗训练和可逆神经网络的负对数密度正则化，以提高生成数据的综合质量。2021年，M. Esmaeilpour 等人[48]提出了一种用于合成包含连续列、二进制列和离散列的表数据集的双判别器GAN。S. Singh 等人[49]提出MeTGAN，使用稀疏线性层来克服CTGAN的内存瓶颈，大大减少了训练的内存使用。Z. Zhao 等人[50]在CTGAN的基础上，提出了CTAB-GAN用于对不同数据类型的建模，包括连续变量、分类变量、混合特征变量等类型。J. Engelmann 等人[52]提出了一种基于条件Wasserstein GAN的方法，对具有数值和分类变量的表格数据集进行建模，并通过一个辅助分类器来特别关注下游分类任务。
DDPM[55]是 J. Ho 等人在2020年提出的扩散概率模型，通过正向的逐步加噪过程将数据分布转化为标准正态分布，并学习逆向去噪过程，从高斯噪声中生成目标数据。该方法通过多步马尔科夫链精确建模生成过程，生成质量高但效率相对较低。在此基础上，A. Kotelnikov 等人[56]提出了一种基于扩散模型的表格数据生成方法TabDDPM，通过特定的噪声添加和去噪过程，有效捕捉了数值型与类别型特征间的复杂关系。
伴随着人工智能和大数据技术的快速发展和应用，对基于生成对抗网络和扩散模型的数据生成方法为近几年的重要方法。然而，当样本中某些数据元素出现缺失时，无法给机器学习任务（如分类、预测等）的模型训练提供更多优质训练样本。因此，需要对样本中的缺失数据元素进行生成填补。目前主要的数据填补方法包括基于统计的填补方法、基于传统机器学习的填补方法、基于深度学习的填补方法等。
```

```
以下仅根据原文中对各文献的简要描述，整理出「[编号] 论文标题/方法名称」列表（原文并未给出文献的完整题目，以下标题/方法名称根据文中表述或常用叫法进行罗列，仅作参考）：

- [35] 自编码器（AE，Autoencoder）  
- [37] 变分自编码器（VAE，Variational Autoencoder）  
- [38] L. Xu 等人提出的对变分自编码器（VAE）的改进方法  
- [39] Generative Adversarial Networks（GAN）  
- [40] Conditional Generative Adversarial Nets（CGANs）  
- [42] Wasserstein GAN（WGAN，基于Wasserstein距离）  
- [43] Wasserstein GAN（WGAN，基于Wasserstein距离）  
- [44] Tabular GAN（TGAN）  
- [45] Conditional Wasserstein Generative Adversarial Nets（CWGAN）  
- [47] 一种广义的GAN框架的表格合成方法（结合GAN与可逆神经网络）  
- [48] 用于合成混合类型表格数据的双判别器GAN  
- [49] MeTGAN（采用稀疏线性层以克服CTGAN内存瓶颈）  
- [50] CTAB-GAN（针对连续、分类、混合等多种特征类型的表格数据建模）  
- [52] 基于条件Wasserstein GAN的表格数据建模方法（带辅助分类器）  
- [55] Denoising Diffusion Probabilistic Models（DDPM，扩散概率模型）  
- [56] TabDDPM（基于扩散模型的表格数据生成方法）  
```

```
[35] Reducing the dimensionality of data with neural networks. Science
@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science},
  abstract={ High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}
}
[37] Auto-encoding variational bayes
@misc{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max and others},
  year={2013},
  publisher={Banff, Canada},
  abstract={How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}
}
[38] Modeling tabular data using conditional gan
@article{xu2019modeling,
  title={Modeling tabular data using conditional gan},
  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  abstract={Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design CTGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. CTGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.}
}
[39] Generative adversarial netss
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014},
  abstract={We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.}
}
[40] Conditional generative adversarial nets. arXiv preprint
@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014},
  abstract={Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.}
}
[42] Wasserstein gan
@article{adler2018banach,
  title={Banach wasserstein gan},
  author={Adler, Jonas and Lunz, Sebastian},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
  abstract={Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered $\ell^2$ as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA.}
}
[43] Towards principled methods for training generative adversarial networks
@inproceedings{
    arjovsky2017towards,
    title={Towards Principled Methods for Training Generative Adversarial Networks},
    author={Martin Arjovsky and Leon Bottou},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=Hk4_qw5xe},
    abstract={The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.}
}
[44] Synthesizing tabular data using generative adversarial nets
@article{xu2018synthesizing,
  title={Synthesizing tabular data using generative adversarial networks},
  author={Xu, Lei and Veeramachaneni, Kalyan},
  journal={arXiv preprint arXiv:1811.11264},
  year={2018},
  abstract={Generative adversarial networks (GANs) implicitly learn the probability distribution of a dataset and can draw samples from the distribution. This paper presents, Tabular GAN (TGAN), a generative adversarial network which can generate tabular data like medical or educational records. Using the power of deep neural networks, TGAN generates high-quality and fully synthetic tables while simultaneously generating discrete and continuous variables. When we evaluate our model on three datasets, we find that TGAN outperforms conventional statistical generative models in both capturing the correlation between columns and scaling up for large datasets.}
}
[45] A conditional Wasserstein generative adversarial network for rolling bearing fault diagnosis
@article{tang2021novel,
  title={A novel intelligent fault diagnosis method for rolling bearings based on Wasserstein generative adversarial network and Convolutional Neural Network under Unbalanced Dataset},
  author={Tang, Hongtao and Gao, Shengbo and Wang, Lei and Li, Xixing and Li, Bing and Pang, Shibao},
  journal={Sensors},
  volume={21},
  number={20},
  pages={6754},
  year={2021},
  publisher={MDPI},
  abstract={Rolling bearings are widely used in industrial manufacturing, and ensuring their stable and effective fault detection is a core requirement in the manufacturing process. However, it is a great challenge to achieve a highly accurate rolling bearing fault diagnosis because of the severe imbalance and distribution differences in fault data due to weak early fault features and interference from environmental noise. An intelligent fault diagnosis strategy for rolling bearings based on grayscale image transformation, a generative adversative network, and a convolutional neural network was proposed to solve this problem. First, the original vibration signal is converted into a grayscale image. Then more training samples are generated using GANs to solve severe imbalance and distribution differences in fault data. Finally, the rolling bearing condition detection and fault identification are carried out by using SECNN. The availability of the method is substantiated by experiments on datasets with different data imbalance ratios. In addition, the superiority of this diagnosis strategy is verified by comparing it with other mainstream intelligent diagnosis techniques. The experimental result demonstrates that this strategy can reach more than 99.6% recognition accuracy even under substantial environmental noise interference or changing working conditions and has good stability in the presence of a severe imbalance in fault data.}
}
[47] A general GAN framework for tabular data synthesis with invertible neural networks
@article{lee2021invertible,
  title={Invertible tabular GANs: Killing two birds with one stone for tabular data synthesis},
  author={Lee, Jaehoon and Hyeong, Jihyeon and Jeon, Jinsung and Park, Noseong and Cho, Jihoon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4263--4273},
  year={2021},
  abstract={Tabular data synthesis has received wide attention in the literature. This is because available data is often limited, incomplete, or cannot be obtained easily, and data privacy is becoming increasingly important. In this work, we present a generalized GAN framework for tabular synthesis, which combines the adversarial training of GANs and the negative log-density regularization of invertible neural networks. The proposed framework can be used for two distinctive objectives. First, we can further improve the synthesis quality, by decreasing the negative log-density of real records in the process of adversarial training. On the other hand, by increasing the negative log-density of real records, realistic fake records can be synthesized in a way that they are not too much close to real records and reduce the chance of potential information leakage. We conduct experiments with real-world datasets for classification, regression, and privacy attacks. In general, the proposed method demonstrates the best synthesis quality (in terms of task-oriented evaluation metrics, e.g., F1) when decreasing the negative log-density during the adversarial training. If increasing the negative log-density, our experimental results show that the distance between real and fake records increases, enhancing robustness against privacy attacks.}
}
[48] A dual-discriminator generative adversarial network for tabular data
@article{nguyen2017dual,
  title={Dual discriminator generative adversarial nets},
  author={Nguyen, Tu and Le, Trung and Vu, Hung and Phung, Dinh},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  abstract={We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.}
}
[49] MeTGAN: A memory-efficient tabular GAN
@inproceedings{singh2021metgan,
  title={Metgan: Memory efficient tabular gan for high cardinality categorical datasets},
  author={Singh, Shreyansh and Kayathwal, Kanishka and Wadhwa, Hardik and Dhama, Gaurav},
  booktitle={Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8--12, 2021, Proceedings, Part VI 28},
  pages={519--527},
  year={2021},
  organization={Springer},
  abstract={Generative Adversarial Networks (GANs) have seen their use for generating synthetic data expand, from unstructured data like images to structured tabular data. One of the recently proposed models in the field of tabular data generation, CTGAN, demonstrated state-of-the-art performance on this task even in the presence of a high class imbalance in categorical columns or multiple modes in continuous columns. Many of the recently proposed methods have also derived ideas from CTGAN. However, training CTGAN requires a high memory footprint while dealing with high cardinality categorical columns in the dataset. In this paper, we propose MeTGAN, a memory-efficient version of CTGAN, which reduces memory usage by roughly 80%, with a minimal effect on performance. MeTGAN uses sparse linear layers to overcome the memory bottlenecks of CTGAN. We compare the performance of MeTGAN with the other models on publicly available datasets. Quality of data generation, memory requirements, and the privacy guarantees of the models are the metrics considered in this study. The goal of this paper is also to draw the attention of the research community on the issue of the computational footprint of tabular data generation methods to enable them on larger datasets especially ones with high cardinality categorical variables.}
}
[50] CTAB-GAN: Effective table data synthesis from mixed data types
@inproceedings{zhao2021ctab,
  title={Ctab-gan: Effective table data synthesizing},
  author={Zhao, Zilong and Kunar, Aditya and Birke, Robert and Chen, Lydia Y},
  booktitle={Asian Conference on Machine Learning},
  pages={97--112},
  year={2021},
  organization={PMLR},
  abstract={While data sharing is crucial for knowledge development, privacy concerns and strict regulation (e.g., European General Data Protection Regulation (GDPR)) unfortunately limit its full effectiveness. Synthetic tabular data emerges as an alternative to enable data sharing while fulfilling regulatory and privacy constraints. The state-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks (GAN) and address two main data types in industry, i.e., continuous and categorical. In this paper, we develop CTAB-GAN, a novel conditional table GAN architecture that can effectively model diverse data types, including a mix of continuous and categorical variables. Moreover, we address data imbalance and long tail issues, i.e., certain variables have drastic frequency differences across large values. To achieve those aims, we first introduce the information loss, classification loss and generator loss to the conditional GAN. Secondly, we design a novel conditional vector, which efficiently encodes the mixed data type and skewed distribution of data variable. We extensively evaluate CTAB-GAN with the state of the art GANs that generate synthetic tables, in terms of data similarity and analysis utility. The results on five datasets show that the synthetic data of CTAB-GAN remarkably resembles the real data for all three types of variables and results into higher accuracy for five machine learning algorithms, by up to 17%.}
}
[52] A conditional Wasserstein GAN approach for tabular data with an auxiliary classifier
@article{engelmann2021conditional,
  title={Conditional Wasserstein GAN-based oversampling of tabular data for imbalanced learning},
  author={Engelmann, Justin and Lessmann, Stefan},
  journal={Expert Systems with Applications},
  volume={174},
  pages={114582},
  year={2021},
  publisher={Elsevier},
  abstract={Class imbalance impedes the predictive performance of classification models. Popular countermeasures include oversampling minority class cases by creating synthetic examples. The paper examines the potential of Generative Adversarial Networks (GANs) for oversampling. A few prior studies have used GANs for this purpose but do not reflect recent methodological advancements for generating tabular data using GANs. The paper proposes an approach based on a conditional Wasserstein GAN that can effectively model tabular datasets with numerical and categorical variables and pays special attention to the down-stream classification task through an auxiliary classifier loss. We focus on a credit scoring context in which binary classifiers predict the default risk of loan applications. Empirical comparisons in this context evidence the competitiveness of GAN-based oversampling compared to several standard oversampling regimes. We also clarify the conditions under which oversampling in general and the proposed GAN-based approach in particular raise predictive performance. In sum, our findings suggest that GAN architectures for tabular data and our extensions deserve a place in data scientists’ modelling toolbox.}
}
[55] Denoising diffusion probabilistic models
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020},
  abstract={We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.}
}
[56] TabDDPM: A denoising diffusion model for tabular data.
@inproceedings{kotelnikov2023tabddpm,
  title={Tabddpm: Modelling tabular data with diffusion models},
  author={Kotelnikov, Akim and Baranchuk, Dmitry and Rubachev, Ivan and Babenko, Artem},
  booktitle={International Conference on Machine Learning},
  pages={17564--17579},
  year={2023},
  organization={PMLR},
  abstract={Denoising diffusion probabilistic models are becoming the leading generative modeling paradigm for many important data modalities. Being the most prevalent in the computer vision community, diffusion models have recently gained some attention in other domains, including speech, NLP, and graph-like data. In this work, we investigate if the framework of diffusion models can be advantageous for general tabular problems, where data points are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data makes it quite challenging for accurate modeling since the individual features can be of a completely different nature, i.e., some of them can be continuous and some can be discrete. To address such data types, we introduce TabDDPM — a diffusion model that can be universally applied to any tabular dataset and handles any feature types. We extensively evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority over existing GAN/VAE alternatives, which is consistent with the advantage of diffusion models in other fields.}
}
```

```
abstract={}
```

```
目前表现比较优秀的数据生成方法主要有基于自编码的生成模型、生成对抗网络（Generative Adversarial Networks, GAN）、扩散模型（Diffusion Probabilistic Models，DDPM）等，旨在通过已有的样本来生成新的样本数据。
基于自编码的生成模型，如：自编码器AE\cite{hinton2006reducing}、变分自编码器VAE\cite{kingma2013auto}等。L. Xu \cite{xu2019modeling}等人提出的是对变分自编码器（VAE）的改进方法，通过对隐变量与表格数据特征的联合分布进行专门建模，以实现对连续和离散特征的有效生成与重构。
GAN\cite{goodfellow2014generative}是 I. J. Goodfellow 等人在2014年提出的模型，GAN受博弈论的启发，内部有生成器（G）和判别器（D）两个网络，两个网络相互对抗博弈达到有效生成数据的目的。M. Mirza 等人\cite{mirza2014conditional}提出的CGANs（Conditional Generative Adversarial Nets）在生成模型和判别模型的建模中均引入条件变量，即数据的标签，将GANs从无监督学习变成有监督学习。M. Arjovsky 等人\cite{adler2018banach,arjovsky2017towards}引入Wasserstein距离来替代JS散度和KL散度，并将其作为优化目标，从而提出了WGAN（Wasserstein GAN），从根本上解决了原始GAN的梯度消失问题。L. Xu 等人\cite{xu2018synthesizing}提出了一种生成对抗网络Tabular GAN (TGAN)，使用LSTM和MLP分别作为生成器和判别器，生成如病例或教育记录等表格数据。2019年，L. Xu 等人\cite{xu2019modeling}提出了一种基于Conditional GAN的CTGAN来生成表格数据，用于对表格数据分布和样本行进行建模。Y. Yu 等人\cite{tang2021novel}提出了一种基于改进的条件生成对抗网络CWGAN(Conditional Wasserstein Generative Adversarial Nets)来学习滚动轴承故障的时频谱特征，并根据输入类别生成相应故障类别的时频谱。J. Lee 等人\cite{lee2021invertible}提出了一个广义的GAN框架的表格合成，它结合了GAN的对抗训练和可逆神经网络的负对数密度正则化，以提高生成数据的综合质量。2021年，M. Esmaeilpour 等人\cite{nguyen2017dual}提出了一种用于合成包含连续列、二进制列和离散列的表数据集的双判别器GAN。S. Singh 等人\cite{singh2021metgan}提出MeTGAN，使用稀疏线性层来克服CTGAN的内存瓶颈，大大减少了训练的内存使用。Z. Zhao 等人\cite{zhao2021ctab}在CTGAN的基础上，提出了CTAB-GAN用于对不同数据类型的建模，包括连续变量、分类变量、混合特征变量等类型。J. Engelmann 等人\cite{engelmann2021conditional}提出了一种基于条件Wasserstein GAN的方法，对具有数值和分类变量的表格数据集进行建模，并通过一个辅助分类器来特别关注下游分类任务。
DDPM\cite{ho2020denoising}是 J. Ho 等人在2020年提出的扩散概率模型，通过正向的逐步加噪过程将数据分布转化为标准正态分布，并学习逆向去噪过程，从高斯噪声中生成目标数据。该方法通过多步马尔科夫链精确建模生成过程，生成质量高但效率相对较低。在此基础上，A. Kotelnikov 等人\cite{kotelnikov2023tabddpm}提出了一种基于扩散模型的表格数据生成方法TabDDPM，通过特定的噪声添加和去噪过程，有效捕捉了数值型与类别型特征间的复杂关系。
伴随着人工智能和大数据技术的快速发展和应用，对基于生成对抗网络和扩散模型的数据生成方法为近几年的重要方法。然而，当样本中某些数据元素出现缺失时，无法给机器学习任务（如分类、预测等）的模型训练提供更多优质训练样本。因此，需要对样本中的缺失数据元素进行生成填补。目前主要的数据填补方法包括基于统计的填补方法、基于传统机器学习的填补方法、基于深度学习的填补方法等。
```

```
生成合成数据的主流策略集中于先进的生成模型，这些模型学习如何从现有数据集中合成新的样本。其中，基于自编码器的方法包括自编码器（AE）\cite{hinton2006reducing}和变分自编码器（VAE）\cite{kingma2013auto}。值得注意的是，\cite{xu2019modeling} 提出了一种基于VAE的增强方法，该方法精确地建模了潜在变量与表格特征的联合分布，从而实现了对连续和离散属性的高效生成和重构。

另一种著名的框架是生成对抗网络（GANs）\cite{goodfellow2014generative}，该框架于2014年提出，灵感来源于博弈论原理。它由两个主要组件组成：生成器（G）和判别器（D），两者在一个极小极大博弈中竞争，以生成逼真的样本。在此基础上，条件生成对抗网络（CGANs）\cite{mirza2014conditional} 将条件变量——通常为标签——融入生成器和判别器中，使GANs从无监督范式转变为有监督范式。与此同时，\cite{adler2018banach,arjovsky2017towards} 在GAN优化中用Wasserstein距离替换了Jensen–Shannon和Kullback–Leibler散度，从而得到了Wasserstein GAN（WGAN）。这一进展有效地解决了传统GANs中梯度消失的问题。

在表格数据领域，\cite{xu2018synthesizing} 提出了TGAN，该方法在生成器和判别器中分别采用长短期记忆网络（LSTM）和多层感知器（MLPs），用于生成合成的表格数据集（例如，医疗或教育记录）。2019年，\cite{xu2019modeling} 提出了CTGAN，一种针对表格数据定制的条件GAN，能够捕捉详细的数据分布并学习生成完整的表格行。类似地，\cite{tang2021novel} 设计了一种增强型条件Wasserstein GAN（CWGAN），用于滚动轴承故障诊断中的时频谱分析，根据输入类别合成故障类型谱图。

后续研究对基于GAN的表格数据生成进行了进一步改进。 \cite{lee2021invertible} 提出了一种可逆的表格GAN框架，将对抗训练与来自可逆神经网络的负对数密度正则化相结合，以提升生成质量。 \cite{nguyen2017dual} 提出了一种双判别器GAN，用于合成包含连续、二值和离散列的数据。针对内存限制问题，\cite{singh2021metgan} 提出了MeTGAN，采用稀疏线性层以缓解CTGAN的高内存使用问题。在扩展CTGAN方面，\cite{zhao2021ctab} 开发了CTAB-GAN，能够有效处理连续、类别和混合特征变量。此外，\cite{engelmann2021conditional} 提出了一种针对具有数值和类别变量的表格数据的条件Wasserstein GAN，并添加了辅助分类器以在下游任务中优先考虑分类结果。

扩散概率模型（DDPMs）\cite{ho2020denoising} 代表了另一种具有影响力的生成方法。通过在前向传递过程中逐步添加高斯噪声，然后通过多步马尔可夫链学习逆向（去噪）过程，DDPMs 能够生成高保真度的样本，尽管效率较低。在此基础上，\cite{kotelnikov2023tabddpm} 提出了针对表格数据的TabDDPM，通过定制噪声添加和去除过程来捕捉数值和类别特征之间的复杂关系。

随着人工智能和大数据解决方案的快速发展和应用，基于GANs和扩散模型的数据生成技术已成为必不可少的工具。然而，这些方法在数据存在缺失值的场景中常常会出现问题，从而降低了用于分类和预测等任务的合成数据的质量。因此，从统计技术到传统机器学习和深度学习方法的稳健插补方法在处理不完整数据时仍然不可或缺。
```

```
\textbf{基于自编码器的方法.}  
这一类别中的先驱性工作是自编码器（AE）\cite{hinton2006reducing}，其训练目的是将高维输入映射到低维潜在编码中，然后从这些编码中重构原始数据。通过强制隐藏（潜在）层降维，AE 有效地学习到了压缩表示。然而，纯粹的 AE 是确定性的，缺乏灵活采样的直接机制。变分自编码器（VAE）\cite{kingma2013auto} 通过引入概率潜在变量框架解决了这一问题，从而可以从学习到的潜在分布中采样出新的、未见过的数据点。这一扩展极大地拓宽了基于自编码器模型在合成数据生成中的潜在应用范围。

在 AE 和 VAE 的基础上，\cite{xu2019modeling} 提出了一种针对表格数据生成和重构的改进方法。其方法更准确地建模了潜在变量与表格特征的联合分布，而后者可能包含连续和离散属性。通过关注不同类型变量之间的相互作用，该方法确保生成的合成数据忠实地反映了真实世界表格数据集（如医疗和教育数据）中存在的复杂依赖关系。

\textbf{生成对抗网络 (GANs).}  
自2014年引入以来，生成对抗网络（GANs）\cite{goodfellow2014generative} 对生成建模领域产生了重大影响。其基本概念基于博弈论：两个网络——生成器（G）和判别器（D）——在对抗循环中训练。判别器学习区分真实数据和合成数据，而生成器则试图生成能够欺骗判别器的样本。当判别器无法再区分真实数据与生成数据时，这个极小极大博弈就告一段落，这表明生成器已经捕捉到了底层分布的统计特征。

\textbf{GANs 的扩展.}  
在 GANs 出现后不久，\cite{mirza2014conditional} 提出了条件生成对抗网络（CGANs），其中生成器和判别器均基于辅助信息（如类别标签或特定输入变量）进行条件化。这一扩展框架使得生成能够针对特定类别或属性进行定向，实际上将原本的无监督设置转变为有监督或半监督范式。  
尽管取得了这些进展，传统 GANs 往往仍然面临梯度消失或模式崩溃的问题。为了解决这些问题，\cite{adler2018banach,arjovsky2017towards} 用 Wasserstein 距离替换了 Jensen–Shannon (JS) 和 Kullback–Leibler (KL) 散度，从而诞生了 Wasserstein GAN (WGAN)。通过利用地球移动者距离（即 Wasserstein 距离），WGANs 被认为能够稳定训练、缓解梯度消失问题，并更好地捕捉具有复杂模式的数据分布。

\textbf{针对表格数据的 GANs.}  
虽然 GANs 最初因图像合成而受到欢迎，但它们在处理通常具有异构特征类型、不平衡和复杂依赖关系的表格数据集时，同样证明既具有挑战性又极具价值。针对这些挑战，\cite{xu2018synthesizing} 提出了表格 GAN（TGAN），该方法在生成器网络中应用了长短期记忆（LSTM）单元，并在判别器中采用了多层感知器（MLPs），从而使深度架构适应于表格数据生成（例如健康记录或学生成绩数据）。2019 年，\cite{xu2019modeling} 提出了 CTGAN，一种专门为处理不平衡离散列、多模态连续列以及表格数据固有复杂性而设计的改进型条件 GAN 架构。通过利用条件采样策略，CTGAN 在建模详细分布和生成与真实数据密切对应的表格行方面表现出色。

\textbf{表格 GANs 的进一步发展.}  
1. \cite{tang2021novel} 提出了一种增强型条件 Wasserstein GAN（CWGAN），用于滚动轴承故障诊断中的时频谱分析。该方法根据输入类别定制合成故障特定的谱图，从而提高了工业诊断任务中的分类和预测性能。  
2. \cite{lee2021invertible} 探索了一种可逆的表格 GAN 框架，该框架将对抗训练与来自可逆神经网络的负对数密度正则化相结合。该方法在训练过程中增加或降低真实样本的对数密度，从而根据隐私和质量目标生成更接近或更远离真实数据流形的合成样本。  
3. 为了解决模式崩溃并提高样本多样性，\cite{nguyen2017dual} 提出了一种双判别器 GAN，该方法结合了 KL 散度和反向 KL 散度，以更好地捕捉真实世界数据分布的多模态特性。  
4. \cite{singh2021metgan} 针对内存限制问题提出了 MeTGAN，在生成器和判别器中采用稀疏线性层，从而显著降低了内存开销，而对合成质量影响不大，这在处理具有高基数类别变量的表格数据集时尤为有利。  
5. 在扩展 CTGAN 功能方面，\cite{zhao2021ctab} 提出了 CTAB-GAN，能够对连续、类别和混合类型的特征进行建模。该方法能够有效处理偏态分布和多样化的特征类型，通常在与真实数据的相似性和下游任务准确性方面优于其他基线方法。  
6. \cite{engelmann2021conditional} 提出了一种针对同时包含数值和类别变量的表格数据的条件 Wasserstein GAN，通过引入辅助分类器，使模型在生成逼真合成数据的同时提升了下游任务（如信用风险评估）中分类器的性能。

\textbf{扩散概率模型 (DDPMs).}  
与 GANs 并行，扩散概率模型（DDPMs）\cite{ho2020denoising} 因其生成高保真样本的能力而受到关注。DDPMs 在多个前向步骤中系统地向数据添加高斯噪声，将原始数据分布转化为一个可处理的先验分布（例如标准正态分布）。随后，该模型通过多步马尔可夫链学习逆向（去噪）过程，逐步去除噪声以恢复数据分布。尽管 DDPMs 在某些领域（尤其是计算机视觉）生成了最先进的合成样本，但其多步特性可能会带来较高的计算开销。  
为了将基于扩散的框架扩展到表格数据，\cite{kotelnikov2023tabddpm} 提出了 TabDDPM，该方法结合了针对表格特征混合性质精心设计的噪声添加与去除调度。通过同时考虑数值和类别变量的分布，TabDDPM 展示了学习多列之间复杂关系的能力，并在生成结构化数据时缓解了纯图像中心扩散方法的缺陷。

\textbf{缺失数据与插补的挑战.}  
尽管 GANs 和 DDPMs 在生成合成数据方面取得了成功，但缺失值对模型训练和下游任务均构成了严峻挑战。例如，不完整的数据可能会影响合成样本的质量，从而降低其在分类或预测应用中的效用。因此，数据插补方法在生成建模过程之前或期间填补部分观测特征中的空缺时显得至关重要。现有的插补策略包括统计技术（例如均值/众数插补）、传统机器学习方法（例如 k 最近邻、随机森林）以及深度学习方法（例如去噪自编码器、循环模型）。将稳健的插补方法与先进的生成方法相结合仍然是一个活跃的研究领域，旨在构建既能处理不完整数据集又能生成信息损失最小的逼真合成数据的系统。

\textbf{结论与展望.}  
从自编码器\cite{hinton2006reducing,kingma2013auto,xu2019modeling}到 GANs\cite{goodfellow2014generative,mirza2014conditional,adler2018banach,arjovsky2017towards,xu2018synthesizing,xu2019modeling,tang2021novel,lee2021invertible,nguyen2017dual,singh2021metgan,zhao2021ctab,engelmann2021conditional}以及扩散概率模型\cite{ho2020denoising,kotelnikov2023tabddpm}，数据生成方法已成为现代人工智能中不可或缺的工具。通过学习生成合理的合成数据，这些方法为隐私保护的数据共享、数据增强和数据驱动洞察的改进提供了支持。尽管如此，处理缺失值仍然是核心问题，其中采用稳健的插补技术对于维护数据完整性和提升生成模型在实际应用中的表现至关重要。未来的研究很可能会探索数据插补、领域适应和生成建模的更紧密结合，进一步提升合成数据在复杂真实环境中的实用性。
```



```

Presently, leading approaches in data generation encompass models such as those rooted in autoencoders, Generative Adversarial Networks (GANs), and Diffusion Probabilistic Models (DDPMs), among others. These techniques are developed to create fresh data samples by leveraging existing ones.

Among the models based on autoencoders are notable examples like the Autoencoder (AE)\cite{hinton2006reducing} and the Variational Autoencoder (VAE)\cite{kingma2013auto}. L. Xu and collaborators\cite{xu2019modeling} introduced an advanced technique enhancing the VAE. Their method focuses on precisely modeling the joint distribution of latent variables and features within tabular data, enabling efficient generation and reconstruction of both continuous and discrete attributes.

The GAN framework\cite{goodfellow2014generative}, unveiled by I. J. Goodfellow and his team in 2014, draws inspiration from game theory principles. It features two core components: a generator (G) and a discriminator (D), which engage in a competitive dynamic to produce realistic data effectively. Building on this, M. Mirza et al.\cite{mirza2014conditional} proposed Conditional Generative Adversarial Networks (CGANs), integrating conditional variables—specifically data labels—into both the generator and discriminator. This innovation shifts GANs from unsupervised to supervised learning paradigms. Meanwhile, M. Arjovsky and colleagues\cite{adler2018banach,arjovsky2017towards} replaced JS and KL divergences with the Wasserstein distance as the optimization target, leading to the development of the Wasserstein GAN (WGAN). This approach fundamentally addresses the gradient vanishing issues inherent in the original GAN design.

L. Xu et al.\cite{xu2018synthesizing} devised Tabular GAN (TGAN), a generative adversarial network employing Long Short-Term Memory (LSTM) units and Multilayer Perceptrons (MLPs) as the generator and discriminator, respectively. TGAN excels at producing tabular datasets, such as medical or educational records. In 2019, L. Xu and associates\cite{xu2019modeling} introduced CTGAN, a Conditional GAN-based model tailored for generating tabular data, adept at capturing data distributions and individual sample rows. Similarly, Y. Yu et al.\cite{tang2021novel} presented an enhanced Conditional Wasserstein GAN (CWGAN), designed to analyze time-frequency spectrum characteristics of rolling bearing faults and synthesize corresponding fault-specific spectra based on input categories.

J. Lee and co-authors\cite{lee2021invertible} proposed a versatile GAN-based framework for tabular data synthesis, merging adversarial training with negative log-density regularization from invertible neural networks to elevate the quality of generated outputs. In 2021, M. Esmaeilpour et al.\cite{nguyen2017dual} developed a dual-discriminator GAN to create tabular datasets comprising continuous, binary, and discrete columns. S. Singh et al.\cite{singh2021metgan} introduced MeTGAN, which utilizes sparse linear layers to mitigate the memory constraints of CTGAN, substantially lowering training memory demands. Extending CTGAN, Z. Zhao et al.\cite{zhao2021ctab} proposed CTAB-GAN, a model adept at handling diverse data types, including continuous, categorical, and mixed-feature variables. Additionally, J. Engelmann et al.\cite{engelmann2021conditional} suggested a conditional Wasserstein GAN approach for modeling tabular datasets with numerical and categorical variables, incorporating an auxiliary classifier to prioritize downstream classification tasks.

The Diffusion Probabilistic Model (DDPM)\cite{ho2020denoising}, introduced by J. Ho and his team in 2020, employs a forward process of incremental noise addition to transform data distributions into a standard normal form. It then learns a reverse denoising mechanism to reconstruct target data from Gaussian noise. This technique leverages a multi-step Markov chain for precise generation modeling, yielding high-quality results albeit with lower efficiency. Building on this foundation, A. Kotelnikov et al.\cite{kotelnikov2023tabddpm} developed TabDDPM, a diffusion-based method for tabular data generation. By applying tailored noise addition and removal processes, TabDDPM effectively captures intricate relationships between numerical and categorical features.

With the swift evolution and adoption of artificial intelligence and big data technologies, data generation strategies relying on GANs and diffusion models have emerged as critical tools in recent years. However, when samples exhibit missing elements, these methods struggle to supply high-quality training data for machine learning applications, such as classification or prediction tasks. Consequently, there is a pressing need to impute these absent data components. Current imputation strategies primarily include statistical techniques, methods grounded in traditional machine learning, and approaches powered by deep learning.

Below are the citations of the papers mentioned in the text, including abstracts, and requests that the original text be refined according to the information provided below to make it more logical, scientific, and correct.

Requirements:
1: All bitex references in the data must be referenced to
2: cite format with latex \cite format, can be one, can also be multiple






@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science},
  abstract={ High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}
}
@misc{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max and others},
  year={2013},
  publisher={Banff, Canada},
  abstract={How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.}
}
@article{xu2019modeling,
  title={Modeling tabular data using conditional gan},
  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019},
  abstract={Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design CTGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. CTGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.}
}
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014},
  abstract={We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.}
}
@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014},
  abstract={Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.}
}
@article{adler2018banach,
  title={Banach wasserstein gan},
  author={Adler, Jonas and Lunz, Sebastian},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018},
  abstract={Wasserstein Generative Adversarial Networks (WGANs) can be used to generate realistic samples from complicated image distributions. The Wasserstein metric used in WGANs is based on a notion of distance between individual images, which induces a notion of distance between probability distributions of images. So far the community has considered $\ell^2$ as the underlying distance. We generalize the theory of WGAN with gradient penalty to Banach spaces, allowing practitioners to select the features to emphasize in the generator. We further discuss the effect of some particular choices of underlying norms, focusing on Sobolev norms. Finally, we demonstrate a boost in performance for an appropriate choice of norm on CIFAR-10 and CelebA.}
}
@inproceedings{
    arjovsky2017towards,
    title={Towards Principled Methods for Training Generative Adversarial Networks},
    author={Martin Arjovsky and Leon Bottou},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=Hk4_qw5xe},
    abstract={The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of gen- erative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first sec- tion introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a prac- tical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.}
}
@article{xu2018synthesizing,
  title={Synthesizing tabular data using generative adversarial networks},
  author={Xu, Lei and Veeramachaneni, Kalyan},
  journal={arXiv preprint arXiv:1811.11264},
  year={2018},
  abstract={Generative adversarial networks (GANs) implicitly learn the probability distribution of a dataset and can draw samples from the distribution. This paper presents, Tabular GAN (TGAN), a generative adversarial network which can generate tabular data like medical or educational records. Using the power of deep neural networks, TGAN generates high-quality and fully synthetic tables while simultaneously generating discrete and continuous variables. When we evaluate our model on three datasets, we find that TGAN outperforms conventional statistical generative models in both capturing the correlation between columns and scaling up for large datasets.}
}
@article{tang2021novel,
  title={A novel intelligent fault diagnosis method for rolling bearings based on Wasserstein generative adversarial network and Convolutional Neural Network under Unbalanced Dataset},
  author={Tang, Hongtao and Gao, Shengbo and Wang, Lei and Li, Xixing and Li, Bing and Pang, Shibao},
  journal={Sensors},
  volume={21},
  number={20},
  pages={6754},
  year={2021},
  publisher={MDPI},
  abstract={Rolling bearings are widely used in industrial manufacturing, and ensuring their stable and effective fault detection is a core requirement in the manufacturing process. However, it is a great challenge to achieve a highly accurate rolling bearing fault diagnosis because of the severe imbalance and distribution differences in fault data due to weak early fault features and interference from environmental noise. An intelligent fault diagnosis strategy for rolling bearings based on grayscale image transformation, a generative adversative network, and a convolutional neural network was proposed to solve this problem. First, the original vibration signal is converted into a grayscale image. Then more training samples are generated using GANs to solve severe imbalance and distribution differences in fault data. Finally, the rolling bearing condition detection and fault identification are carried out by using SECNN. The availability of the method is substantiated by experiments on datasets with different data imbalance ratios. In addition, the superiority of this diagnosis strategy is verified by comparing it with other mainstream intelligent diagnosis techniques. The experimental result demonstrates that this strategy can reach more than 99.6% recognition accuracy even under substantial environmental noise interference or changing working conditions and has good stability in the presence of a severe imbalance in fault data.}
}
@article{lee2021invertible,
  title={Invertible tabular GANs: Killing two birds with one stone for tabular data synthesis},
  author={Lee, Jaehoon and Hyeong, Jihyeon and Jeon, Jinsung and Park, Noseong and Cho, Jihoon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4263--4273},
  year={2021},
  abstract={Tabular data synthesis has received wide attention in the literature. This is because available data is often limited, incomplete, or cannot be obtained easily, and data privacy is becoming increasingly important. In this work, we present a generalized GAN framework for tabular synthesis, which combines the adversarial training of GANs and the negative log-density regularization of invertible neural networks. The proposed framework can be used for two distinctive objectives. First, we can further improve the synthesis quality, by decreasing the negative log-density of real records in the process of adversarial training. On the other hand, by increasing the negative log-density of real records, realistic fake records can be synthesized in a way that they are not too much close to real records and reduce the chance of potential information leakage. We conduct experiments with real-world datasets for classification, regression, and privacy attacks. In general, the proposed method demonstrates the best synthesis quality (in terms of task-oriented evaluation metrics, e.g., F1) when decreasing the negative log-density during the adversarial training. If increasing the negative log-density, our experimental results show that the distance between real and fake records increases, enhancing robustness against privacy attacks.}
}
@article{nguyen2017dual,
  title={Dual discriminator generative adversarial nets},
  author={Nguyen, Tu and Le, Trung and Vu, Hung and Phung, Dinh},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  abstract={We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.}
}
@inproceedings{singh2021metgan,
  title={Metgan: Memory efficient tabular gan for high cardinality categorical datasets},
  author={Singh, Shreyansh and Kayathwal, Kanishka and Wadhwa, Hardik and Dhama, Gaurav},
  booktitle={Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8--12, 2021, Proceedings, Part VI 28},
  pages={519--527},
  year={2021},
  organization={Springer},
  abstract={Generative Adversarial Networks (GANs) have seen their use for generating synthetic data expand, from unstructured data like images to structured tabular data. One of the recently proposed models in the field of tabular data generation, CTGAN, demonstrated state-of-the-art performance on this task even in the presence of a high class imbalance in categorical columns or multiple modes in continuous columns. Many of the recently proposed methods have also derived ideas from CTGAN. However, training CTGAN requires a high memory footprint while dealing with high cardinality categorical columns in the dataset. In this paper, we propose MeTGAN, a memory-efficient version of CTGAN, which reduces memory usage by roughly 80%, with a minimal effect on performance. MeTGAN uses sparse linear layers to overcome the memory bottlenecks of CTGAN. We compare the performance of MeTGAN with the other models on publicly available datasets. Quality of data generation, memory requirements, and the privacy guarantees of the models are the metrics considered in this study. The goal of this paper is also to draw the attention of the research community on the issue of the computational footprint of tabular data generation methods to enable them on larger datasets especially ones with high cardinality categorical variables.}
}
@inproceedings{zhao2021ctab,
  title={Ctab-gan: Effective table data synthesizing},
  author={Zhao, Zilong and Kunar, Aditya and Birke, Robert and Chen, Lydia Y},
  booktitle={Asian Conference on Machine Learning},
  pages={97--112},
  year={2021},
  organization={PMLR},
  abstract={While data sharing is crucial for knowledge development, privacy concerns and strict regulation (e.g., European General Data Protection Regulation (GDPR)) unfortunately limit its full effectiveness. Synthetic tabular data emerges as an alternative to enable data sharing while fulfilling regulatory and privacy constraints. The state-of-the-art tabular data synthesizers draw methodologies from Generative Adversarial Networks (GAN) and address two main data types in industry, i.e., continuous and categorical. In this paper, we develop CTAB-GAN, a novel conditional table GAN architecture that can effectively model diverse data types, including a mix of continuous and categorical variables. Moreover, we address data imbalance and long tail issues, i.e., certain variables have drastic frequency differences across large values. To achieve those aims, we first introduce the information loss, classification loss and generator loss to the conditional GAN. Secondly, we design a novel conditional vector, which efficiently encodes the mixed data type and skewed distribution of data variable. We extensively evaluate CTAB-GAN with the state of the art GANs that generate synthetic tables, in terms of data similarity and analysis utility. The results on five datasets show that the synthetic data of CTAB-GAN remarkably resembles the real data for all three types of variables and results into higher accuracy for five machine learning algorithms, by up to 17%.}
}
@article{engelmann2021conditional,
  title={Conditional Wasserstein GAN-based oversampling of tabular data for imbalanced learning},
  author={Engelmann, Justin and Lessmann, Stefan},
  journal={Expert Systems with Applications},
  volume={174},
  pages={114582},
  year={2021},
  publisher={Elsevier},
  abstract={Class imbalance impedes the predictive performance of classification models. Popular countermeasures include oversampling minority class cases by creating synthetic examples. The paper examines the potential of Generative Adversarial Networks (GANs) for oversampling. A few prior studies have used GANs for this purpose but do not reflect recent methodological advancements for generating tabular data using GANs. The paper proposes an approach based on a conditional Wasserstein GAN that can effectively model tabular datasets with numerical and categorical variables and pays special attention to the down-stream classification task through an auxiliary classifier loss. We focus on a credit scoring context in which binary classifiers predict the default risk of loan applications. Empirical comparisons in this context evidence the competitiveness of GAN-based oversampling compared to several standard oversampling regimes. We also clarify the conditions under which oversampling in general and the proposed GAN-based approach in particular raise predictive performance. In sum, our findings suggest that GAN architectures for tabular data and our extensions deserve a place in data scientists’ modelling toolbox.}
}
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020},
  abstract={We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.}
}
@inproceedings{kotelnikov2023tabddpm,
  title={Tabddpm: Modelling tabular data with diffusion models},
  author={Kotelnikov, Akim and Baranchuk, Dmitry and Rubachev, Ivan and Babenko, Artem},
  booktitle={International Conference on Machine Learning},
  pages={17564--17579},
  year={2023},
  organization={PMLR},
  abstract={Denoising diffusion probabilistic models are becoming the leading generative modeling paradigm for many important data modalities. Being the most prevalent in the computer vision community, diffusion models have recently gained some attention in other domains, including speech, NLP, and graph-like data. In this work, we investigate if the framework of diffusion models can be advantageous for general tabular problems, where data points are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data makes it quite challenging for accurate modeling since the individual features can be of a completely different nature, i.e., some of them can be continuous and some can be discrete. To address such data types, we introduce TabDDPM — a diffusion model that can be universally applied to any tabular dataset and handles any feature types. We extensively evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority over existing GAN/VAE alternatives, which is consistent with the advantage of diffusion models in other fields.}
}
```

