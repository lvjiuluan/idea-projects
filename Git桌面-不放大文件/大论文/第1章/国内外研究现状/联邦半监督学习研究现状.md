# 原文

```
当前的联邦半监督学习方法主要基于横向联邦架构（即参与方共享特征，但样本ID不同）。根据标注数据的位置分为标签在客户端和标签在服务器端两种情况\cite{jin2023federated}。
（1）标签在客户端
标注数据存于客户端，而服务器只能获取未标注数据。例如，某家公司欲利用智能手机拍摄的图片训练一个物体检测的联邦学习模型，但无法直接访问用户的本地数据，只能依赖用户的标注(见图4.1)。首先，对于不同的参与方（用户），样本ID是不同的，但参与方（用户）手机里的图片特征是一样的，所以构成了横向联邦的设置。其次，用户通常不愿为每张照片标注，给基于横向联邦的半监督学习创造了一个"客户端标注"（label-at-client）的环境。

RSCFe\cite{liang2022rscfed} 主要关注联邦半监督学习中的标签隔离问题和数据异质性问题。在局部训练中，采用师生模型\cite{tarvainen2017mean}对无标签数据进行训练。为进一步解决数据异质性问题，RSCFed 提出了子共识抽样法和距离加权聚合法。在每一轮中，通过对所有参与者的多个子集进行独立抽样，从而聚合出多个子共识模型，这样每个子共识模型都有望包含拥有标记数据的参与者。此外，本地模型会根据它们与子共识模型的距离进行加权，这样偏差模型就会得到较低的权重，其影响也会降到最低。
FedSSL\cite{fan2022private}解决了标签隔离问题、数据隐私问题和数据异构问题。为了便于对未标记的客户端进行本地训练，FedSSL 利用了伪标记技术。此外，为解决数据异构问题，FedSSL 学习全局生成模型，从统一的特征空间生成数据，从而通过生成的数据缓解数据异构问题。最后，为了防止生成模型造成的隐私泄露，FedSSL 利用差分隐私（DP）来限制生成模型中训练数据的信息泄露。
FedMatch\cite{jeong2020federated}提出了一种客户端间一致性损失来解决数据异质性问题。具体来说，对每个客户端的前k个最近客户端进行采样，在每个数据样本上，本地模型的输出与前k个客户端模型的输出进行正则化，以确保一致性。此外，FedMatch 还提出了分离式学习方法，将标注数据和未标注数据的参数分开，未标注数据的参数是稀疏的。更新时，只有未标记数据的客户端会上传稀疏张量，从而降低通信成本。
FedPU\cite{lin2022federated}研究了半监督学习中更具挑战性的环境--正向和无标签学习，在这种环境中，每个客户端只有类的子集标签。在这种情况下，客户端只掌握所有类别中的一部分信息，从而导致严重的标签隔离问题。为了解决这个问题，FedPU 提出了一个新颖的目标函数，即把学习客户端负类的任务交给拥有负类标签数据的其他客户端。这样，每个客户端只负责学习正类，并可自行进行局部训练。根据经验，在正类和无标签学习设置中，所提出的 FedPU 优于 FedMatch\cite{jeong2020federated}。
AdaFedSemi\cite{wang2022enhancing}提出了一种系统，可在联合半监督学习中利用服务器端无标记数据实现效率与模型准确性之间的权衡。在每一轮学习中，模型都是通过客户端的标注数据进行训练，并在服务器端进行汇总。服务器端未标注数据通过伪标注纳入训练过程。AdaFedSemi\cite{wang2022enhancing}确定了平衡效率和性能的两个关键参数，即客户端参与率P和伪标签的置信度阈值τ。较低的 P可以降低通信成本和模型准确性，而较高的τ可以降低服务器端计算成本，同时也限制了未标记数据的使用。。实验表明，AdaFedSemi 通过动态调整 P 和τ在不同的训练阶段实现了效率和准确性之间的良好平衡。
DS-FL\cite{itahara2021distillation}解决了与 AdaFedSemi 类似的问题，即客户端拥有标签数据，而服务器拥有非标签数据。它提出了一种集合伪标签解决方案来利用服务器端的非标签数据。具体来说，它不是对数据样本使用单一的伪标签，而是对所有客户端生成的伪标签进行平均，。这将创建一个客户端模型集合，并提供更好的性能。此外，由于只传输伪标签而不是模型参数，通信成本可以大大降低。此外，DS-FL 发现在伪标签上进行训练会导致预测熵增大。因此，DS-FL 提出了一种减少熵的聚合方法，即在聚合之前使局部输出。
（2）标签在服务器端
标注数据存于服务器，而客户端只有未标注数据。例如，一家可穿戴设备公司希望利用联邦学习训练健康监测模型（见图4.2）。在这种情况下，由于用户通常缺乏专业知识，无法标注健康相关数据，因此客户端的数据是未标注的。
标签数据在客户端的情况比标签在服务器端设置更复杂，原因是所有客户端都只拥有未标记数据，无法为联邦模型提供额外的监督信号。如文献\cite{jeong2020federated,diao2022semifl}所示，仅使用无标签数据进行训练可能会导致遗忘从有标签数据中学到的知识，从而影响模型性能。
为了解决标签数据和非标签数据之间的隔离问题，FedMatch\cite{jeong2020federated}提出了一种不相干的学习方案，即分别为标签数据和非标签数据设置两套参数。在非标签数据上进行训练时，标签数据的参数是固定的，反之亦然，以防止知识被覆盖。非标记数据的参数会在参与者和服务器之间传输，而非标记数据的参数设置为稀疏的，这为通信效率带来了额外的好处。此外，为了解决不同客户端持有的异构数据问题，FedMatch 提出了客户端间一致性损失，这样不同参与者的本地模型就能在相同数据上产生相似的输出。
SemiFL\cite{diao2022semifl}采用另一种方法来解决这些挑战。它建议使用标注数据对全局模型进行微调，以提高其质量，并减轻客户端无监督训练所造成的遗忘。此外，SemiFL 建议最大限度地提高客户端模型与全局模型之间的一致性，而不是在客户端之间对模型输出进行正则化。具体来说，全局模型为客户端的未标记数据生成伪标签，而客户端的局部模型则根据伪标签进行训练。实证结果表明，与 FedMatch 相比，SemiFL能产生更有竞争力的结果。
（3）联邦半监督学习方法总结
表4.1总结了前文介绍的基于横向联邦的半监督学习方法，按照标签在客户端/服务器端划分。

```



# bitex

```
@article{jin2023federated,
  title={Federated learning without full labels: A survey},
  author={Jin, Yilun and Liu, Yang and Chen, Kai and Yang, Qiang},
  journal={arXiv preprint arXiv:2303.14453},
  year={2023}
}
@inproceedings{liang2022rscfed,
  title={Rscfed: Random sampling consensus federated semi-supervised learning},
  author={Liang, Xiaoxiao and Lin, Yiqun and Fu, Huazhu and Zhu, Lei and Li, Xiaomeng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10154--10163},
  year={2022}
}
@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{fan2022private,
  title={Private Semi-Supervised Federated Learning.},
  author={Fan, Chenyou and Hu, Junjie and Huang, Jianwei},
  booktitle={IJCAI},
  pages={2009--2015},
  year={2022}
}
@article{jeong2020federated,
  title={Federated semi-supervised learning with inter-client consistency \& disjoint learning},
  author={Jeong, Wonyong and Yoon, Jaehong and Yang, Eunho and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2006.12097},
  year={2020}
}
@inproceedings{lin2022federated,
  title={Federated learning with positive and unlabeled data},
  author={Lin, Xinyang and Chen, Hanting and Xu, Yixing and Xu, Chao and Gui, Xiaolin and Deng, Yiping and Wang, Yunhe},
  booktitle={International Conference on Machine Learning},
  pages={13344--13355},
  year={2022},
  organization={PMLR}
}
@inproceedings{wang2022enhancing,
  title={Enhancing federated learning with in-cloud unlabeled data},
  author={Wang, Lun and Xu, Yang and Xu, Hongli and Liu, Jianchun and Wang, Zhiyuan and Huang, Liusheng},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)},
  pages={136--149},
  year={2022},
  organization={IEEE}
}
@article{itahara2021distillation,
  title={Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data},
  author={Itahara, Sohei and Nishio, Takayuki and Koda, Yusuke and Morikura, Masahiro and Yamamoto, Koji},
  journal={IEEE Transactions on Mobile Computing},
  volume={22},
  number={1},
  pages={191--205},
  year={2021},
  publisher={IEEE}
}
@article{diao2022semifl,
  title={Semifl: Semi-supervised federated learning for unlabeled clients with alternate training},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17871--17884},
  year={2022}
}
```



```
[42] Diao, E., J. Ding, and V.J.A.i.N.I.P.S. Tarokh, SemiFL: Semi-supervised federated learning for unlabeled clients with alternate training. 2022. 35: p. 17871-17884.
@article{diao2022semifl,
  title={Semifl: Semi-supervised federated learning for unlabeled clients with alternate training},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17871--17884},
  year={2022}
}


[43] Lee, D.-H. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. in Workshop on challenges in representation learning, ICML. 2013. Atlanta.
@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013},
  organization={Atlanta}
}



[44] Bekker, J. and J.J.M.L. Davis, Learning from positive and unlabeled data: A survey. 2020. 109: p. 719-760.、、、
@article{bekker2020learning,
  title={Learning from positive and unlabeled data: A survey},
  author={Bekker, Jessa and Davis, Jesse},
  journal={Machine Learning},
  volume={109},
  number={4},
  pages={719--760},
  year={2020},
  publisher={Springer}
}


[45] Bachman, P., O. Alsharif, and D.J.A.i.n.i.p.s. Precup, Learning with pseudo-ensembles. 2014. 27.
@article{bachman2014learning,
  title={Learning with pseudo-ensembles},
  author={Bachman, Philip and Alsharif, Ouais and Precup, Doina},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

[46] Ganin, Y., et al., Domain-adversarial training of neural networks. 2016. 17(1): p. 2096-2030.

@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and March, Mario and Lempitsky, Victor},
  journal={Journal of machine learning research},
  volume={17},
  number={59},
  pages={1--35},
  year={2016}
}
```

