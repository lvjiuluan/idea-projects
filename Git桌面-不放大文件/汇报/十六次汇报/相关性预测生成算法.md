以下内容将对所述方法作更加形式化、论文风格的描述，并在叙述中融入相应的数学符号与推导，使之在科学性、严谨性和逻辑性上更符合学术论文的规范。

---

## 摘要（Abstract）
在纵向联邦学习场景中，常常存在 A 方（拥有完整数据集）和 B 方（数据规模较小且部分缺失）的情况。为避免仅因 B 方数据行数较少而在样本对齐中丢弃 A 方多余的数据，提出一种结合“基于相关性的列选择”与“生成模型”的混合方法：对与 A 方强相关的 B 方列，采用纵向联邦半监督的方法进行预测，从而补全 B 方缺失的行；对于弱相关的列，则采用生成模型进行数据补全。本文通过数学化的形式，对上述方法进行系统性的描述，并说明其在实际应用中的可行性及优势。

---

## 1. 引言（Introduction）
在真实的数据合作场景中，经常面临如下挑战：  
1. **样本对齐困难**：由于参与方（如 A 方与 B 方）在样本数量上不一致，无法直接对齐并充分利用 A 方的全部数据。  
2. **缺失值与数据不完整**：B 方的部分数据列存在缺失值，并且 B 方整体行数少于 A 方。  
3. **数据分布差异与强/弱相关性并存**：并非 B 方所有特征都与 A 方特征存在强相关关系，若对弱相关列也强行采用联邦预测，可能带来负面效果或无明显收益。

为此，我们提出如下策略：  
1. **基于列相关性进行选择**：仅对与 A 方相关性足够高的 B 方列，使用纵向联邦半监督方法进行预测补全。  
2. **生成模型进行补全**：对于与 A 方相关性低的列，采用生成式模型（例如 CTGAN）进行数据合成或补全，以降低模型训练和推断的复杂度。  
3. **多余行的保留与 B 方数据扩充**：对 B 方原始行数无法覆盖到的 A 方额外行，亦通过联邦预测或生成模型的方式进行扩充，形成“B_filled”，从而保证最终输出时，A 和 B 的行数一致。

---

## 2. 问题定义与符号（Problem Definition and Notation）

令  
- \(\mathbf{X}^A \in \mathbb{R}^{N_A \times d_A}\) 表示 A 方的数据矩阵，具有 \(N_A\) 条样本、\(d_A\) 个特征；  
- \(\mathbf{X}^B \in \mathbb{R}^{N_B \times d_B}\) 表示 B 方的数据矩阵，具有 \(N_B\) 条样本、\(d_B\) 个特征，其中 \(N_B < N_A\)。  

另外，A 方和 B 方各自的数据列可用如下方式索引：  
- A 方第 \(i\) 个特征向量记为 \(\mathbf{x}_i^A \in \mathbb{R}^{N_A}\)；  
- B 方第 \(j\) 个特征向量记为 \(\mathbf{x}_j^B \in \mathbb{R}^{N_B}\)。

由于 B 方存在部分缺失值（行或列上的缺失），记  
- \(\mathbf{x}_j^B\) 中存在缺失的条目集合为 \(\Omega_j \subset \{1,2,\dots,N_B\}\)；  
- 对于尚未对齐的 A 方额外行，可视为索引集合 \(\Lambda \subset \{1,2,\dots,N_A\}\)，其中 \(|\Lambda| = N_A - N_B\)。  

我们的目标是：  
1. 对 B 方缺失数据进行补全，包括缺失条目和缺失行；  
2. 尽可能保留 A 方多余的样本，使最终生成的 B 方数据（记为 \(\mathbf{X}^{B_{\text{filled}}}\)）与 \(\mathbf{X}^A\) 在行维度对齐，即最终 \(\mathbf{X}^{B_{\text{filled}}} \in \mathbb{R}^{N_A \times d_B}\)。

---

## 3. 方法描述（Methodology）

本节将详细阐述我们的混合策略，包括基于相关性选择列以及利用纵向联邦半监督方法预测和生成模型补全的流程。

### 3.1 计算列相关性（Column-wise Correlation Computation）
对于 B 方的每个特征 \(\mathbf{x}_j^B\)，先将其与 A 方完整数据 \(\mathbf{X}^A\) 进行相关性度量。可以选用 Pearson 相关系数或互信息（MI, Mutual Information）等衡量指标。以 Pearson 相关系数为例：

\[
\rho_{j,i} = \frac{\sum_{m=1}^{n_j} (x_{j,m}^B - \bar{x}_j^B)(x_{i,m}^A - \bar{x}_i^A)}{\sqrt{\sum_{m=1}^{n_j}(x_{j,m}^B - \bar{x}_j^B)^2}\,\sqrt{\sum_{m=1}^{n_j}(x_{i,m}^A - \bar{x}_i^A)^2}}
\]

其中  
- \(n_j \leq N_B\) 表示参与该相关性计算的有效对齐样本数（存在缺失值或无法对齐时需剔除），  
- \(\bar{x}_j^B\) 与 \(\bar{x}_i^A\) 分别是对应特征的样本均值。

再结合所有 \(\rho_{j,i}\) 的最大绝对值或平均值，即可得到 B 方第 \(j\) 列整体对 A 方数据的“强关联程度”指标。若该指标大于预设阈值 \(\delta\)，则判定为“强相关”；否则为“弱相关”。

### 3.2 对强相关列的纵向联邦半监督预测（Vertical Federated Semi-supervised Prediction）

令满足强相关条件的 B 方列集合为 \(\mathcal{J}_{\text{strong}}\)。对于其中的任意一列 \(j \in \mathcal{J}_{\text{strong}}\)，我们做如下处理：

1. **训练样本准备**  
   - 输入：A 方完整数据 \(\mathbf{X}^A\) 中与第 \(j\) 列强相关的那些特征 \(\mathbf{x}_i^A\)（可选取所有特征，或仅选取相关性显著的特征）作为自变量 \(\mathbf{X}^A_{\text{train}}\)。  
   - 标签：B 方对应特征 \(\mathbf{x}_j^B\) 的对齐部分作为目标变量 \(\mathbf{y}^B_j\)。  
   由于 B 方数据行数少于 A 方，因此有一部分 A 方行与 B 方并不对齐，或缺失了 B 方该列的值。对于可对齐的那部分（行数记作 \(n_j\)），可以组成 \((\mathbf{X}^A_{\text{train}}, \mathbf{y}^B_j)\) 用于建模。

2. **纵向联邦半监督学习**  
   令 \(\mathbf{M}(\cdot)\) 表示一个在联邦架构下训练的模型，典型做法是：  
   - 在 A 方本地执行特征向量变换或加密；  
   - 在 B 方或协同中心对标签/半监督信息进行同态加密后的梯度更新；  
   - 模型训练结束后，可在 A 方完成推断，以保护隐私。  
   **半监督** 指的是如果有部分样本（A 方多出来的 \(\Lambda\)）缺少 B 方标签，则在训练时将其视作无标注数据，仅利用分布信息来辅助训练（例如一致性正则、伪标签等方法）。该过程可简单表达为：  
   \[
     \min_{\theta} \sum_{(x,y)\in \text{Labeled}} \ell(\mathbf{M}(x;\theta), y) \;+\; \alpha \sum_{x\in \text{Unlabeled}} R(\mathbf{M}(x;\theta))
   \]  
   其中 \(\ell\) 为有监督的损失函数，\(R\) 为半监督正则项，\(\alpha\) 为平衡系数。

3. **预测补全**  
   训练完成后，对 A 方多余的 \(\Lambda\) 行进行推断，得到 \(\hat{\mathbf{x}}_{j,\Lambda}^B\)。从而，可将原本只在 B 方有 \(N_B\) 行的数据列 \(\mathbf{x}_j^B\)，扩展至 \(\mathbf{x}_{j}^{B_{\text{filled}}} \in \mathbb{R}^{N_A}\)，即使 B 方对应行并不存在（或缺失），也完成了预测补足。  
   将此新列 \(\mathbf{x}_j^{B_{\text{filled}}}\) 合并至 B_filled 矩阵的第 \(j\) 列位置。

### 3.3 对弱相关列的生成模型补全（Generation-based Completion）

对于弱相关列 \(\mathcal{J}_{\text{weak}}\)，即对 A 方数据整体相关性较低者，则不再使用联邦半监督预测。一方面，若强行训练可能导致高方差或无效的预测；另一方面，也会增加额外的通信与计算成本。对此，我们直接采用生成模型（如 CTGAN）来补全缺失值及扩展行数：  
1. **训练 CTGAN（或其他生成模型）**：将 B 方已有行的部分观测值作为训练数据，学习到其分布参数；  
2. **生成新样本**：根据需要生成 \(|\Lambda|\) 行新的模拟样本，以补足 B 方在行数上的缺失；  
3. **插值/补全**：对已存在的行中缺失值则可使用同一生成模型进行有条件生成，或通过插补策略结合真实分布进行填充。  

这样得到的 \(\mathbf{x}_j^{B_{\text{gen}}}\) 不依赖 A 方信息，因此即便 A 与 B 的相关性不高，也能保证数据一致性并尽量保留 B 方特征分布。

### 3.4 完整数据的输出

通过上面两步处理后，B 方数据可拆分为两部分：  
- **\(B_{\text{filled}}\)**：对强相关列采用纵向联邦预测完成扩充，使行数达到 \(N_A\)；  
- **\(B_{\text{rest}}\)**：对弱相关列采用生成模型部分或直接保留原有行。若需要可额外生成新行，但不再依赖 A 方做精确预测。  

最终输出包含：  
1. **\(\mathbf{X}^A\)**：A 方完整数据。  
2. **\(\mathbf{X}^{B_{\text{filled}}}\)**：行数与 \(\mathbf{X}^A\) 对齐、经过预测和生成补全后得到的 B 方数据。  
3. **\(\mathbf{X}^{B_{\text{rest}}}\)**：可选输出，仅对缺失列或弱相关列进行补全后剩余的“B 方自有部分”或“生成部分”，行数与 \(\mathbf{X}^{B_{\text{filled}}}\) 未必相同（如不强制扩充到 \(N_A\) 的场景）。

---

## 4. 算法流程小结（Algorithmic Summary）

下表概括了整个流程（仅示意）：

1. **输入**: \(\mathbf{X}^A\), \(\mathbf{X}^B\), 相关性阈值 \(\delta\)  
2. **对 B 方各列 \(j = 1,\dots,d_B\)**:  
   a) 计算与 A 方数据的相关性指标 \(\text{Corr}_j\)。  
   b) 若 \(\text{Corr}_j \geq \delta\)（强相关）: 
      - 使用纵向联邦半监督学习在 A 方的特征空间进行训练；  
      - 预测生成 \(\mathbf{x}_j^{B_{\text{filled}}}\) 并使行数扩展到 \(N_A\)。  
   c) 若 \(\text{Corr}_j < \delta\)（弱相关）: 
      - 使用生成模型对缺失行/缺失值进行补全，得到 \(\mathbf{x}_j^{B_{\text{gen}}}\)。  
3. **输出**:  
   - \(\mathbf{X}^A\)；  
   - 拼接各列后得到的 \(\mathbf{X}^{B_{\text{filled}}}\)，其行数与 \(\mathbf{X}^A\) 相同；  
   - 其余部分 \(\mathbf{X}^{B_{\text{rest}}}\)（若需要保留）。

---

## 5. 实验与讨论（Experiments and Discussion）
> *本节为论文标准框架，具体实验方案与结果因题意暂未给出，这里仅做简要说明。*

1. **实验数据准备**  
   - 选取若干模拟数据集及真实数据集，在其中对 B 方随机丢失若干行和若干列值。  
   - 设定不同的相关性阈值 \(\delta\)，探讨阈值对最终补全效果的影响。

2. **评价指标**  
   - **预测精度**：对于强相关列，使用 RMSE、MAE 等评估联邦半监督预测对缺失行的补全质量；  
   - **生成分布一致性**：对于弱相关列，衡量 CTGAN 等生成后分布与真实分布的偏差（如 MMD、K-S检验、Wasserstein distance 等）；  
   - **保留行数**：比较方法之间对 A 方多余行的利用情况，量化丢弃行数与实际保留情况。

3. **结果分析**  
   - 当 B 方列与 A 方强相关时，联邦半监督预测在保证隐私的同时能更好地还原缺失行的真实数值；  
   - 当列相关性不足时，强行联邦预测难以提升效果，生成模型可更灵活地进行缺失值填补；  
   - 方法整体在减少无效联邦通信和计算量的同时，维持或提升了数据补全的精度与行数的保留率。

---

## 6. 结论（Conclusion）
本文面向纵向联邦学习场景下的缺失数据补全问题，提出了一种基于列相关性的混合策略：  
- 首先计算 B 方各列与 A 方数据的相关性，区分强相关与弱相关；  
- 对强相关列，利用纵向联邦半监督模型进行预测，并补全 B 方在行数和缺失值上的缺口；  
- 对弱相关列，则直接采用生成模型（CTGAN）等方法进行数据合成和补全。  

该方法在减少模型与特征维度冗余、降低联邦学习通信与计算开销的同时，充分利用 A 方额外行数据，显著提高了 B 方数据的完整度，且保留了更多 A 方可用的样本。为不同实际应用场景（如金融、医疗等需隐私保护且存在数据规模差异的场景）提供了可行且有效的解决方案。

---

### 参考文献（References）
> 此处列出若干可能用到的基础文献示例：
> 1. Goodfellow, I., Pouget-Abadie, J., Mirza, M., et al. (2014). Generative Adversarial Nets. *Advances in Neural Information Processing Systems (NIPS)*.  
> 2. Xu, L., Veeramachaneni, K. (2018). Synthesizing Tabular Data using Generative Adversarial Networks. *arXiv preprint arXiv:1811.11264*.  
> 3. McMahan, B., Moore, E., Ramage, D., & Hampson, S. (2017). Communication-Efficient Learning of Deep Networks from Decentralized Data. *Proc. of AISTATS*.  
> 4. Oliver, A., Odena, A., Raffel, C., Cubuk, E. D., & Goodfellow, I. (2018). Realistic Evaluation of Deep Semi-Supervised Learning Algorithms. *Advances in Neural Information Processing Systems (NeurIPS)*.  

---

以上即为论文形式的系统化方法描述。从整体流程到数学符号、再到实验评估和结论，均遵循了学术论文的常见逻辑与结构。通过该方法，能够在充分利用 A 方大规模数据的前提下，为 B 方补全缺失行与缺失值，同时借助相关性分析尽量避免对弱相关列做多余的模型训练，兼顾效率与效果。