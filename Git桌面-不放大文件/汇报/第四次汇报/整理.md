# 问题描述

<img src="D:\Typora\images\image-20240718091446605.png" alt="image-20240718091446605" style="zoom:80%;" />

## B方部分列有特征缺失

<img src="D:\Typora\images\image-20240721150652697.png" alt="image-20240721150652697" style="zoom:80%;" />

# 解决方案

- 全部用gan网路生成
- （1）==一列一列全部用半监督（循环）==，（2）或者一次性做半监督同时的生成（结合gan网络，一次性生成）
- 先用gan网络生成一部分，再用半监督生成一部分。

# 一列一列全部用半监督（循环）

把B方特征缺失的部分单独拿出来：

<img src="D:\Typora\images\image-20240721150736646-17226454891711.png" alt="image-20240721150736646" style="zoom:80%;" />

对于B方特征缺失的第$i$列，$i=  1,2,3...,k$，将其作为标签列:

<img src="D:\Typora\images\image-20240721150912626.png" alt="image-20240721150912626" style="zoom:80%;" />



目标：利用A方和B方对齐的数据集，进行联邦学习，预测出unlabeled的标签

VFPU:

labeled: 全为1

unlabled：有1和0

取1，剩下的未取的

样本越多

采用VFPU，哪个类别做正样本，哪个类别做负样本，数据集中正样本负样本比例是否对结果产生影响需要验证

之所以用VFPU，是因为数据集中标签正负样本极不均衡，所以才用随机抽样的方式形成样本均衡的二分类数据集，然后用纵向的分类器学习，



正负样本占比

对于$b_i$,有以下三种情况：

- 只有两个类别的分类列
- 多个类别的分类列
- 连续的数值列

## 只有两个类别的分类列


$$
\eqalign{
  & B_{defciency} = \{ {b_1},{b_2},...,{b_k}\}   \cr 
  & {b_i} \in \{ 0,1 \}  \cr}
$$

思路：使用VFPU方法进行半监督学习，预测unlabled标签

## 多个类别的分类列

思路：将多个类别的转换到只有两个类别的情况

考虑$N$个类别$${C_1},{C_2},...,{C_N}$$，多分类学习的基本思路是“拆解法”，即将多分类任务拆解为若干个二分类任务。具体地，先对原问题进行拆分，然后为拆分出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。由以上分析可知，关键是如何拆分以及如何集成。对于给定数据集:
$$
\eqalign{
  & B = \{ {b_1},{b_2},...,{b_k}\}   \cr 
  & {b_i} \in \{ {C_1},{C_2},...,{C_N}\}  \cr}
$$

<img src="https://img2020.cnblogs.com/blog/2429903/202108/2429903-20210825143953707-165947671.png" alt="img" style="zoom: 67%;" />

验证：如果C1和C2比较接近，把这两个比较接近的类别先分成一大类，到底在划分类别的时候从哪里去划分，在分这个C1、C2，最优切分点的问题，集合

## 连续的数值列

将连续的数值离散化：

分箱

cart回归

其它的一些算法

2 新的算法：纵向联邦半监督回归算法

思路1：将连续的数值列转换到多个类别的分类列的情况

连续属性离散化
   离散化 (Discretization) (有些时候叫 量化(quantization) 或 分箱(binning)) ，是将连续特征划分为离散特征值的方法。 离散化可以把具有连续属性的数据集变换成只有名义属性(nominal attributes)的数据集。

​		K-bins 离散化（分箱）
   KBinsDiscretizer 类使用k个等宽的bins把特征离散化：默认情况下，输出是被 one-hot 编码到一个稀疏矩阵。(请看类别特征编码)。 而且可以使用参数encode进行配置。对每一个特征， bin的边界以及总数目在 fit过程中被计算出来，它们将用来定义区间。

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer

# 将目标值离散化
kb = KBinsDiscretizer(n_bins=10, strategy='uniform', encode='ordinal')
y_binned = kb.fit_transform(y.reshape(-1, 1))

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y_binned.ravel(), test_size=0.2, random_state=42)

# 创建模型
classifier = RandomForestClassifier()

# 训练模型
classifier.fit(X_train, y_train)

# 预测
y_pred = classifier.predict(X_test)

# 计算准确率
acc = accuracy_score(y_test, y_pred)
print("Accuracy: ", acc)

# 打印混淆矩阵
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix: \n", cm)


# 将预测结果转回连续值
y_pred_continuous = kb.inverse_transform(y_pred.reshape(-1, 1))

# 计算均方误差
mse = mean_squared_error(y_test, y_pred_continuous)
print("Mean Squared Error: ", mse)

```



思路2：设计一种新的纵向联邦半监督回归算法

### 回归树 （Regression Tree）

说到决策树（Decision tree），我们很自然会想到用其做分类，每个叶子代表有限类别中的一个。但是对于决策树解决回归问题，一直是一知半解，很多时候都是一带而过。

对于一个回归问题，我们第一时间想到的可能就是线性回归（linear regression），当线性回归不好的时候，可能想着用 SVR（Support Vector Regression）试试。但回归树（regression tree）也很重要，现在 shallow learning 被 SVM 和树模型统治，随机森林、GBDT、xgboost、lightGBM 大行其道，所以知道什么是回归树很有必要。

常用的决策树有 ID3、C4.5、CART 等，其中 CART 就可以用来做回归问题，CART 全称就是 Classification And Regression Tree（分类和回归树）。

### 回归树定义

回归树（regression tree），顾名思义，就是用树模型做回归问题，每一片叶子都输出一个预测值。预测值一般是该片叶子所含训练集元素输出的均值，即${c_m} = ave({y_i}|{x_i} \in lea{f_m})$。

CART 在分类问题和回归问题中的相同和差异：

- 相同：
  - 在分类问题和回归问题中，CART 都是一棵二叉树，除叶子节点外的所有节点都有且仅有两个子节点；
  - 所有落在同一片叶子中的输入都有同样的输出。
- 差异：
  - 在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。
  - 在分类问题中，CART 的每一片叶子都代表的是一个 class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。

下面以 criteria = 'mse' 为例，介绍 CART 回归树。

### 理论解释

给定一个数据集，$D = {\kern 1pt} \{ ({x_1},{y_1}),({x_2},{y_2}),...,({x_n},{y_n})\} $，其中$x_i$是一个m维的向量，即$x_i$含有m个features。

回归问题的目标就是构造一个函数$f(x)$能够你和$D$中的元素，使得mse最小，即：
$$
\min {1 \over n}\sum\limits_{i = 1}^n {{{(f({x_i}) - {y_i})}^2}} 
$$
用 CART 进行回归，目标自然也是一样的，最小化 mse。

假设一棵构建好的 CART 回归树有 $M$ 片叶子，这意味着 CART 将输入空间 $x$ 划分成了$M$ 个单元 $${R_1},{R_2},...,{R_M}$$ 同时意味着 CART 至多会有 $M$  个不同的预测值。CART 最小化 mse 公式如下：
$$
\min {1 \over n}\sum\limits_{m = 1}^M {\sum\limits_{{x_i} \in {R_m}} {{{({c_m} - {y_i})}^2}} }
$$
其中，$c_m$ 表示第 $m$ 片叶子的预测值。
想要最小化 CART 总体的 mse，只需要最小化每一片叶子的 mse 即可，而最小化一片叶子的 mse，只需要将预测值设定为叶子中含有的训练集元素的均值，即：
$$
{c_m} = ave({y_i}|{x_i} \in lea{f_m})
$$
所以，在每一次的划分中，选择切分变量（splitting variable）和切分点（splitting point）时（也就是选择 feature 和将该 feature space 一分为二的 split），使得模型在训练集上的 mse 最小，也就是每片叶子的 mse 之和最小。

这里采用启发式的方法，遍历所有的切分变量和切分点，然后选出 *叶子节点 mse 之和最小* 的那种情况作为划分。选择第 $j$ 个 feature $${x^{(j)}}$$ 和它取的值 $s$，作为切分变量和切分点，则切分变量和切分点将父节点的输入空间一分为二：
$$
\eqalign{
  & {R_1}\{ j,s\}  = \{ x|{x^{(j)}} \le s\}   \cr 
  & {R_2}{\rm{\{ j,s\}   =  \{ x|}}{{\rm{x}}^{(j)}}{\rm{ > s\} }} \cr}
$$
CART 选择切分变量 $j$ 和 切分点 $s$ 的公式如下：


$$
\mathop {\min }\limits_{j,s} \left[ {\mathop {\min }\limits_{{c_1}} \sum\limits_{{x_i} \in {R_1}(j,s)} {{{({y_i} - {c_1})}^2}}  + \mathop {\min }\limits_{{c_2}} \sum\limits_{{x_i} \in {R_2}(j,s)} {{{({y_i} - {c_2})}^2}} } \right]
$$


采取遍历的方式，我们可以将$j$ 和 $s$ 找出来：先固定 feature $j$ 再选出在该 feature 下的最佳划分$s$；对每一个 feature 都这样做，那么有 $m$ 个feature，我们就能得到 $m$ 个 feature 对应的最佳划分，从这$m$ 个值中取最小值即可得到令全局最优的 $(j,s)$。第一项 $${\mathop {\min }\limits_{{c_1}} \sum\limits_{{x_i} \in {R_1}(j,s)} {{{({y_i} - {c_1})}^2}} }$$得到的 $c_1$ 值按照 $$ave({y_i}|{x_i} \in {R_1}\{ j,s\} )$$取的。同理，第二项中$${\mathop {\min }\limits_{c2} \sum\limits_{{x_i} \in {R_2}(j,s)} {{{({y_i} - {c_2})}^2}} }$$ 的$c_2$计算方式是：$$ave({y_i}|{x_i} \in {R_2}\{ j,s\} )$$。



![image-20240706220442009](D:\Typora\images\image-20240706220442009.png)

### 遇到的问题

1、将多分类、回归问题转为现有的解决方案

2、从VPFU的角度，改造VFPU这个方法使其多分类、连序列