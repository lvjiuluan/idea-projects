把整个流程以文档的形式梳理一下



# 回归树 （Regression Tree）

说到决策树（Decision tree），我们很自然会想到用其做分类，每个叶子代表有限类别中的一个。但是对于决策树解决回归问题，一直是一知半解，很多时候都是一带而过。

对于一个回归问题，我们第一时间想到的可能就是线性回归（linear regression），当线性回归不好的时候，可能想着用 SVR（Support Vector Regression）试试。但回归树（regression tree）也很重要，现在 shallow learning 被 SVM 和树模型统治，随机森林、GBDT、xgboost、lightGBM 大行其道，所以知道什么是回归树很有必要。

常用的决策树有 ID3、C4.5、CART 等，其中 CART 就可以用来做回归问题，CART 全称就是 Classification And Regression Tree（分类和回归树）。

## 回归树定义

回归树（regression tree），顾名思义，就是用树模型做回归问题，每一片叶子都输出一个预测值。预测值一般是该片叶子所含训练集元素输出的均值，即${c_m} = ave({y_i}|{x_i} \in lea{f_m})$。

CART 在分类问题和回归问题中的相同和差异：

- 相同：
  - 在分类问题和回归问题中，CART 都是一棵二叉树，除叶子节点外的所有节点都有且仅有两个子节点；
  - 所有落在同一片叶子中的输入都有同样的输出。
- 差异：
  - 在分类问题中，CART 使用基尼指数（Gini index）作为选择特征（feature）和划分（split）的依据；在回归问题中，CART 使用 mse（mean square error）或者 mae（mean absolute error）作为选择 feature 和 split 的 criteria。
  - 在分类问题中，CART 的每一片叶子都代表的是一个 class；在回归问题中，CART 的每一片叶子表示的是一个预测值，取值是连续的。

下面以 criteria = 'mse' 为例，介绍 CART 回归树。

## 理论解释

给定一个数据集，$D = {\kern 1pt} \{ ({x_1},{y_1}),({x_2},{y_2}),...,({x_n},{y_n})\} $，其中$x_i$是一个m维的向量，即$x_i$含有m个features。

回归问题的目标就是构造一个函数$f(x)$能够你和$D$中的元素，使得mse最小，即：
$$
\min {1 \over n}\sum\limits_{i = 1}^n {{{(f({x_i}) - {y_i})}^2}} 
$$
用 CART 进行回归，目标自然也是一样的，最小化 mse。

假设一棵构建好的 CART 回归树有 $M$ 片叶子，这意味着 CART 将输入空间 $x$ 划分成了$M$ 个单元 $${R_1},{R_2},...,{R_M}$$ 同时意味着 CART 至多会有 $M$  个不同的预测值。CART 最小化 mse 公式如下：
$$
\min {1 \over n}\sum\limits_{m = 1}^M {\sum\limits_{{x_i} \in {R_m}} {{{({c_m} - {y_i})}^2}} }
$$
其中，$c_m$ 表示第 $m$ 片叶子的预测值。
想要最小化 CART 总体的 mse，只需要最小化每一片叶子的 mse 即可，而最小化一片叶子的 mse，只需要将预测值设定为叶子中含有的训练集元素的均值，即：
$$
{c_m} = ave({y_i}|{x_i} \in lea{f_m})
$$
所以，在每一次的划分中，选择切分变量（splitting variable）和切分点（splitting point）时（也就是选择 feature 和将该 feature space 一分为二的 split），使得模型在训练集上的 mse 最小，也就是每片叶子的 mse 之和最小。

这里采用启发式的方法，遍历所有的切分变量和切分点，然后选出 *叶子节点 mse 之和最小* 的那种情况作为划分。选择第 $j$ 个 feature $${x^{(j)}}$$ 和它取的值 $s$，作为切分变量和切分点，则切分变量和切分点将父节点的输入空间一分为二：
$$
\eqalign{
  & {R_1}\{ j,s\}  = \{ x|{x^{(j)}} \le s\}   \cr 
  & {R_2}{\rm{\{ j,s\}   =  \{ x|}}{{\rm{x}}^{(j)}}{\rm{ > s\} }} \cr}
$$
CART 选择切分变量 $j$ 和 切分点 $s$ 的公式如下：


$$
\mathop {\min }\limits_{j,s} \left[ {\mathop {\min }\limits_{{c_1}} \sum\limits_{{x_i} \in {R_1}(j,s)} {{{({y_i} - {c_1})}^2}}  + \mathop {\min }\limits_{{c_2}} \sum\limits_{{x_i} \in {R_2}(j,s)} {{{({y_i} - {c_2})}^2}} } \right]
$$


采取遍历的方式，我们可以将$j$ 和 $s$ 找出来：先固定 feature $j$ 再选出在该 feature 下的最佳划分$s$；对每一个 feature 都这样做，那么有 $m$ 个feature，我们就能得到 $m$ 个 feature 对应的最佳划分，从这$m$ 个值中取最小值即可得到令全局最优的 $(j,s)$。第一项 $${\mathop {\min }\limits_{{c_1}} \sum\limits_{{x_i} \in {R_1}(j,s)} {{{({y_i} - {c_1})}^2}} }$$得到的 $c_1$ 值按照 $$ave({y_i}|{x_i} \in {R_1}\{ j,s\} )$$取的。同理，第二项中$${\mathop {\min }\limits_{c2} \sum\limits_{{x_i} \in {R_2}(j,s)} {{{({y_i} - {c_2})}^2}} }$$ 的$c_2$计算方式是：$$ave({y_i}|{x_i} \in {R_2}\{ j,s\} )$$。



![image-20240706220442009](D:\Typora\images\image-20240706220442009.png)

![Figure 1 in JEPG format](D:\Typora\images\Figure 1 in JEPG format.jpg)
