现实中会遇到多分类任务。虽然我们可以用神经网络直接建模多分类问题，但在机器学习的早期，一般模型只能解决二分类问题，因此有必要了解如何将二分类问题推广到多分类问题。

# 思路



考虑$N$个类别$${C_1},{C_2},...,{C_N}$$，多分类学习的基本思路是“拆解法”，即将多分类任务拆解为若干个二分类任务。具体地，先对原问题进行拆分，然后为拆分出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。由以上分析可知，关键是如何拆分以及如何集成。对于给定数据集:
$$
\eqalign{
  & D = \{ ({x_1},{y_1}),({x_2},{y_2}),...,({x_m},{y_m})\}   \cr 
  & {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {\kern 1pt} {y_i} \in \{ {C_1},{C_2},...,{C_N}\}  \cr} 
$$
一般的拆分策略有三种：

- 一对一（OvO）
- 一对其余（OvR）
- 多对多（MvM）

# 一对一

OvO将N个类别两两配对，从而产生N(N−1)/2个二分类任务，例如OvO将为区分类别$C_i$和$C_j$训练一个分类器，该分类器把$D$中的$C_i$类样例作为正例，$C_j$类样例作为反例。在测试阶段，新样本同时交给所有分类器，于是可得到N(N−1)/2个分类结果，把预测的最多的类别作为最终分类结果。

# 一对其余

OvR每次将一个类别的样例作为正例、所有其他类的样例作为反例，这样可以训练出$N$个分类器。在测试阶段若只有一个分类器预测为正例，则对应的类别标记作为最终分类结果。若有多个分类器预测为正例，则通常考虑各分类器预测的置信度，将置信度最大的类别标记作为最终分类结果。

![img](https://img2020.cnblogs.com/blog/2429903/202108/2429903-20210825143953707-165947671.png)

上图是OvO和OvR的示意图。易知，OvR需要训练N个分类器，OvO需要训练N(N-1)/2个分类器，素偶i，OvO的存储开销和测试开销比OvR更大。但在训练阶段，OvO的没个分类器只用到了一部分数据集，而OvR的每个分类器都用到了全部的数据集，因此，OvO的训练开销比OvR更小。在预测性能方面，两者通常是差不多的。

# 多对多

MvM每次将若干个类作为正类，若干个其他类作为反类。MvM的正反类构造通常用一种成为“纠错输出吗”（ECOC）的技术。ECOC是将编码的思想引入类别拆分中，并尽可能在解码过程中具有容错性。ECOC的工作过程主要分为两步：

- 编码：对N个类别做M此划分，每次划分将一部分类别划为正类，一部分划为反类，形成一个二分类训练集；这样一共产生M个训练集，可训练出M个分类器。
- 解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码，将这个预测编码与每个类别各自编码进行比较，返回其中距离最小的类别最为最终类别。