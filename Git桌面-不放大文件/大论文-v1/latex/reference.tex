\begin{thebibliography}{200}
	\wuhao %设置参考文献字体大小
	\linespread{1}\selectfont
	\setlength{\itemsep}{-1.4ex} %缩小条目间行距
	\thispagestyle{others}
	\pagestyle{others}
	
	\makeatletter
	\renewcommand\@biblabel[1]{[#1]\hfill} %序号左对齐
	\makeatother
	\setlength{\labelsep}{0cm}
	
	
	\bibitem{chen2021secureboost+}
	FAN, CHEN, MA et al. SecureBoost+: Large Scale and\&nbsp;High-Performance Vertical Federated Gradient Boosting Decision Tree[C]. Advances in Knowledge Discovery and Data Mining: 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2024, Taipei, Taiwan, May 7–10, 2024, Proceedings, Part III, Berlin, Heidelberg, 2024: 237–249.

	\bibitem{de2010practical}
	CRISTOFARO D, TSUDIK, GENE. Practical Private Set Intersection Protocols with Linear Complexity[C]. Financial Cryptography and Data Security: 14th International Conference, FC 2010, Tenerife, Canary Islands, January 25-28, 2010, Revised Selected Papers 14, Berlin, Heidelberg, 2010: 143-159.

	\bibitem{li2021comatch}
	HOI SCH. CoMatch: Semi-Supervised Learning with Contrastive Graph Regularization[C]. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), Los Alamitos, CA, USA, 2021: 9475-9484.

	\bibitem{jin2023federated}
	TSOUVALAS, SAEED, OZCELEBI et al. Labeling Chaos to Learning Harmony: Federated Learning with Noisy Labels[J]. ACM Trans. Intell. Syst. Technol., 2024, 15(2): 26.

	\bibitem{liang2022rscfed}
	LIANG, LIN, FU et al. RSCFed: Random Sampling Consensus Federated Semi-Supervised Learning[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, New Orleans, LA, USA, 2022: 10154-10163.

	\bibitem{tarvainen2017mean}
	TARVAINEN, VALPOLA, HARRI. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results[C]. Advances in Neural Information Processing Systems, Red Hook, NY, USA, 2017: 1195-1204.

	\bibitem{fan2022private}
	FAN, HU, HUANG et al. Private Semi-Supervised Federated Learning[C]. Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, Vienna, Austria, 2022: 2009-2015.

	\bibitem{jeong2020federated}
	ZHENG, TANG, JU et al. 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)[C]. 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), 2024: 4054-4059.

	\bibitem{wang2022enhancing}
	WANG, XU, XU et al. Enhancing Federated Learning with In-Cloud Unlabeled Data[C]. 2022 IEEE, Kuala Lumpur, Malaysia, 2022: 136-149.

	\bibitem{itahara2021distillation}
	ITAHARA, NISHIO, KODA et al. Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data[J]. IEEE Transactions on Mobile Computing, 2021, 22(1): 191-205.

	\bibitem{diao2022semifl}
	DIAO, DING, TAROKH et al. Semifl: Semi-supervised federated learning for unlabeled clients with alternate training[C]. Advances in Neural Information Processing Systems, New Orleans, LA, USA, 2022: 17871-17884.

	\bibitem{hinton2006reducing}
	HINTON, SALAKHUTDINOV, R. R. Reducing the Dimensionality of Data with Neural Networks[J]. Science, 2006, 313(5786): 504-507.

	\bibitem{kingma2013auto}
	CHEN, LIU, PENG et al. Auto-encoding variational Bayes[J]. Cambridge Explorations in Arts and Sciences, 2024, 2(1).

	\bibitem{xu2019modeling}
	XU, SKOULARIDOU, CUESTA-INFANTE et al. Modeling Tabular Data Using Conditional GAN[C]. Advances in Neural Information Processing Systems, Vancouver, Canada, 2019: 7333-7343.

	\bibitem{goodfellow2014generative}
	GOODFELLOW, POUGET-ABADIE, MIRZA et al. Generative Adversarial Nets[C]. Advances in Neural Information Processing Systems, Montreal, Canada, 2014: 2672-2680.

	\bibitem{mirza2014conditional}
	MIRZA, OSINDERO, SIMON. Conditional Generative Adversarial Nets[J]. Computer Science, 2014: 2672-2680.

	\bibitem{adler2018banach}
	ADLER, LUNZ, SEBASTIAN. Banach Wasserstein[C]. Advances in Neural Information Processing Systems, La Jolla, CA, USA, 2018: 6754-6763.

	\bibitem{arjovsky2017towards}
	BOTTOU L. Towards Principled Methods for Training Generative Adversarial Networks[C]. International Conference on Learning Representations, Toulon, France, 2017.

	\bibitem{xu2018synthesizing}
	LIU, FAN, LI et al. Tabular data synthesis with generative adversarial networks: design space and optimizations[J]. The VLDB Journal, 2023, 33(2): 255–280.

	\bibitem{lee2021invertible}
	LEE, HYEONG, JEON et al. Invertible Tabular GAN[C]. Advances in Neural Information Processing Systems, Red Hook, NY, USA, 2021: 4263-4273.

	\bibitem{nguyen2017dual}
	NGUYEN, LE, VU et al. Dual discriminator generative adversarial nets[C]. Advances in Neural Information Processing Systems, Red Hook, NY, USA, 2017: 2670-2680.

	\bibitem{singh2021metgan}
	SINGH, KAYATHWAL, WADHWA et al. Metgan: Memory efficient tabular gan for high cardinality categorical datasets[C]. Neural Information Processing: 28th International Conference, ICONIP 2021, Sanur, Bali, Indonesia, December 8--12, 2021, Proceedings, Part VI, Cham, Switzerland, 2021: 519-527.

	\bibitem{zhao2021ctab}
	ZHAO, KUNAR, BIRKE et al. Ctab-gan: Effective table data synthesizing[C]. Asian Conference on Machine Learning, Virtual, 2021: 97-112.

	\bibitem{yang2019federated}
	YANG, LIU, CHEN et al. Federated Machine Learning: Concept and Applications[J]. ACM Transactions on Intelligent Systems and Technology (TIST), 2019, 10(2): 1-19.

	\bibitem{mcmahan2017communication}
	MCMAHAN, MOORE, RAMAGE et al. Communication-Efficient Learning of Deep Networks from Decentralized Data[C]. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics AISTATS, Fort Lauderdale, FL, USA, 2017: 1273-1282.

	\bibitem{konevcny2016federated}
	KONEVCNY, MCMAHAN, YU et al. Federated Learning: Strategies for Improving Communication Efficiency[J]. arXiv preprint, 2016, abs/1610.05492.

	\bibitem{li2020federated}
	LI, SAHU, ZAHEER et al. Federated Optimization in Heterogeneous Networks[J]. Proceedings of Machine Learning and Systems, 2020, 2: 429-450.

	\bibitem{kairouz2021advances}
	KAIROUZ, MCMAHAN, AVENT et al. Advances and Open Problems in Federated Learning[J]. Foundations and Trends\textregistered, 2021, 14(1--2): 1-210.

	\bibitem{konevcny2015federated}
	LI, LI, ZHANG et al. Federated Adam-Type Algorithm for Distributed Optimization With Lazy Strategy[J]. IEEE Internet of Things Journal, 2022, 9(20): 20519-20531.

	\bibitem{liu2019communication}
	LIU, HUANG, LI et al. Communication-efficient federated learning for wireless edge intelligence: A survey[C]. 2019 IEEE 20th International Conference on Communication Technology (ICCT), Xi'an, China, 2019: 1327-1332.

	\bibitem{liu2020secure}
	LIU, KANG, XING et al. A secure federated transfer learning framework[J]. IEEE Intelligent Systems, 2020, 35(4): 70-82.

	\bibitem{chen2020vafl}
	CHEN, YIN, LI et al. VaFL: A method of vertical asynchronous federated learning for heterogeneous data distribution[C]. 2020 International Conference on Machine Learning and Cybernetics (ICMLC), Adelaide, Australia, 2020: 1-7.

	\bibitem{2015Semi}
	RASMUS, VALPOLA, HONKALA et al. Semi-Supervised Learning with Ladder Networks[J]. Computer Science, 2015, 9 Suppl 1(1): 1-9.

	\bibitem{van2020survey}
	ENGELEN v, HOOS, H. H. A survey on semi-supervised learning[J]. Machine Learning, 2020, 109(2): 373-440.

	\bibitem{lee2013pseudo}
	LEE, DONG-HYUN. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks[C]. ICML Workshop on Challenges in Representation Learning, Atlanta, GA, USA, 2013: 896-903.

	\bibitem{elkan2008learning}
	ELKAN, NOTO, KEITH. Learning classifiers from only positive and unlabeled data[C]. Proceedings of the 14th ACM, New York, NY, USA, 2008: 213-220.

	\bibitem{mordelet2013bagging}
	MORDELET, VERT, JEAN-PHILIPPE. Bagging for positive and unlabeled learning[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35(10): 2402-2412.

	\bibitem{li2021survey}
	LI, ZHANG, CHEN et al. A Survey on Tabular Data Generation Techniques[J]. IEEE Transactions on Knowledge and Data Engineering, 2021, 33(12): 6213-6232.

	\bibitem{zhang2020tab}
	ZHANG, WU, DU et al. TAB: A Hybrid Framework for Multi-dimensional Table Synthesis[C]. Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, New York, NY, USA, 2020: 1234-1241.

	\bibitem{brown2019differential}
	BROWN, WILLIAMS, DAVIS et al. Differentially Private Synthetic Tabular Data Generation via Deep Generative Models[C]. Proceedings of the 2019 IEEE International Conference on Data Mining (ICDM), Beijing, China, 2019: 567-576.

	\bibitem{lin2022federated}
	LIN, CHEN, XU et al. Federated Learning with Positive and Unlabeled Data[C]. International Conference on Machine Learning, Baltimore, MD, USA, 2022: 13344-13355.

	\bibitem{mordelet2014bagging}
	MORDELET, VERT, J-P. A bagging SVM to learn from positive and unlabeled examples[J]. Pattern Recognition Letters, 2014, 37(1): 201-209.

	\bibitem{liu2003building}
	LIU, DAI, LI et al. Building text classifiers using positive and unlabeled examples[C]. Proceedings of the Third IEEE International Conference on Data Mining, Melbourne, FL, USA, 2003: 179-186.

	\bibitem{liu2015classification}
	LIU, TAO, DACHENG. Classification with Noisy Labels by Importance Reweighting[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 38(3): 447-461.

	\bibitem{xu2017multi}
	XU, XU, XU et al. Multi-Positive and Unlabeled Learning[C]. Proceedings of the 26th International Joint Conference on Artificial Intelligence, Melbourne, Australia, 2017: 3182-3188.

	\bibitem{he2021secure}
	HE, DU, ZHU et al. Secure logistic regression for vertical federated learning[J]. IEEE Internet Computing, 2021, 26(2): 61-68.

	\bibitem{yang2019parallel}
	LIU SYaBRaXZaL. Parallel Distributed Logistic Regression for Vertical Federated Learning
	without Third-Party Coordinator[J]. Clinical Orthopaedics and Related Research, 2019, abs/1911.09824.

	\bibitem{yao2022efficient}
	GENCTURK, SINACI, CICEKLI et al. BOFRF: A novel boosting-based federated random forest algorithm on horizontally partitioned data[J]. IEEE Access, 2022, 10: 89835-89851.

	\bibitem{xu2021efficient}
	HAN, CHENG, ZHAO et al. SecureXGB: A Secure and Efficient Multi-party Protocol for Vertical Federated XGBoost[J]. Proc. ACM Manag. Data, 2025, 3(1): 26.

	\bibitem{wang2022feverless}
	WANG, ERSOY, OUG. Feverless: Fast and Secure Vertical Federated Learning Based on XGBoost for Decentralized Labels[J]. IEEE Transactions on Big Data, 2022, 9(1): 295-308.

	\bibitem{feng2019securegbm}
	FENG, XIONG, SONG et al. Securegbm: Secure multi-party gradient boosting[C]. 2019 IEEE International Conference on Big Data (Big Data), Los Angeles, CA, USA, 2019: 1312-1321.

	\bibitem{fitriani2021data}
	FITRIANI, FEBRIANTO, CANDRA D. Data mining for potential customer segmentation in the marketing bank dataset[J]. JUITA: Jurnal Informatika, 2021, 9(1): 25-32.

	\bibitem{subasi2019prediction}
	SUBASI, CANKURT, SELCUK. Prediction of default payment of credit card clients using Data Mining Techniques[C]. 2019 International Engineering Conference (IEC), Istanbul, Turkey, 2019: 115-120.

	\bibitem{chakrabarty2018statistical}
	CHAKRABARTY, BISWAS, SANKET. A statistical approach to adult census income level prediction[C]. 2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN), Greater Noida, India, 2018: 207-212.

	\bibitem{aono2016scalable}
	AONO, HAYASHI, PHONG T et al. Scalable and secure logistic regression via homomorphic encryption[C]. Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy, New York, NY, USA, 2016: 142-144.

	\bibitem{li2022fedtree}
	LI, ZHAOMIN, CAI et al. Proceedings of Machine Learning and Systems[C]. Proceedings of Machine Learning and Systems, 2023: 89-103.

	\bibitem{liu2021fate}
	LIU, FAN, CHEN et al. FATE: An Industrial Grade Platform for Collaborative Learning with Data Protection[J]. Journal of Machine Learning Research, 2021, 22(226): 1-27.

	\bibitem{pedregosa2011scikit}
	PEDREGOSA, VAROQUAUX, GA\"E. Scikit-learn: Machine Learning in P[J]. Journal of Machine Learning Research, 2011, 12: 2825-2830.

	\bibitem{chen2015xgboost}
	CHEN, HE, BENESTY et al. Xgboost: extreme gradient boosting[J]. R package version 0.4-2, 2015, 1(4): 1-4.

	\bibitem{ke2017lightgbm}
	KE, MENG, FINLEY et al. Lightgbm: A highly efficient gradient boosting decision tree[C]. Advances in Neural Information Processing Systems, Red Hook, NY, USA, 2017: 3146-3154.

	\bibitem{paillier1999public}
	PAILLIER, PASCAL. Public-Key Cryptosystems Based on Composite Degree Residuosity Classes[C]. Advances in Cryptology---EUROCRYPT '99, Berlin, Heidelberg, 1999: 223-238.

	\bibitem{cheng2021secureboost}
	CHENG, FAN, JIN et al. Secureboost: A Lossless Federated Learning Framework[J]. IEEE Intelligent Systems, 2021, 36(6): 87-98.

	\bibitem{sohn2020fixmatch}
	SOHN, BERTHELOT, LI et al. FixMatch: simplifying semi-supervised learning with consistency and confidence[C]. Proceedings of the 34th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2020: 13.

	\bibitem{oliver2018realistic}
	OLIVER, ODENA, RAFFEL et al. Realistic evaluation of deep semi-supervised learning algorithms[C]. Proceedings of the 32nd International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2018: 3239–3250.

	\bibitem{liu2023multi}
	LIU, LV, CHEN et al. Multi-Party Federated Recommendation Based on Semi-Supervised Learning[J]. IEEE Transactions on Big Data, 2023, 10(4): 356-370.

	\bibitem{serbian}
	RADAKOVI M, MARJANOVI M, RISTI I, et al.The Serbian Sign Language Alphabet: A Unique Authentic Dataset of Letter Sign Gestures[J].Mathematics (2227-7390), 2024, 12(4).

	\bibitem{news}
	ANDRÉ ARAUJO, CHAVES J , CHEN D et al.Stanford I2V: a news video dataset for query-by-image experiments[J].Proceedings of the 6th ACM Multimedia Systems Conference, 2015.
	
	\bibitem{TabDDPM}
	KOTELNIKOV A, BARANCHUK D, RUBACHEV I, et al.TabDDPM: Modelling Tabular Data with Diffusion Models[J].  2022.DOI:10.48550/arXiv.2209.15421.
	
	\bibitem{MRFs}
	GHASIMI P, BENEDIKTSSON J A, ULFARSSON M O. The spectral-spatial classification of hyperspectral images based on Hidden Markov Random Field and its Expectation-Maximization[C]//Geoscience \& Remote Sensing Symposium.IEEE, 2014.DOI:10.1109/IGARSS.2013.6721358.
	
	\bibitem{VertiGAN}
	Jiang X, Zhang Y, Zhou X, et al. Distributed gan-based privacy-preserving publication of vertically-partitioned data[J]. Proceedings on Privacy Enhancing Technologies, 2023.
	
	\bibitem{CTGAN}
	XU L, CUESTA-INFANTE A, SKOULARIDOU M, et al.Modeling Tabular Data using Conditional GAN[C]//Advances in Neural Information Processing Systems 32, Volume 10 of 20: 32nd Conference on Neural Information Processing Systems (NeurIPS 2019).Vancouver(CA).8-14 December 2019.2020.
	
	\bibitem{TableGAN}
	PARK N, MOHAMMADI M, GORDE K, et al.Data Synthesis based on Generative Adversarial Networks[C]//VLDB 2018.2018.
	
	\bibitem{CTABGAN}
	ZHAO Z, KUNAR A, SCHEER H V D, et al.CTAB-GAN: Effective Table Data Synthesizing[J].  2021.DOI:10.48550/arXiv.2102.08369.
	
	\bibitem{kotelnikov2023tabddpm}
	KOTELNIKOV, BARANCHUK, RUBACHEV et al. TabDDPM: Modelling Tabular Data with Diffusion Models[C]. International Conference on Machine Learning, Honolulu, Hawaii, USA, 2023: 17564-17579.
	
	\bibitem{GAIN}
	YOON J, JORDON J, SCHAAR M V D. GAIN: Missing Data Imputation using Generative Adversarial Nets[C]//2018.DOI:10.48550/arXiv.1806.02920.
	
	\bibitem{VGAIN}
	MIAO X, WU Y, CHEN L, et al.An Experimental Survey of Missing Data Imputation Algorithms[J].IEEE Transactions on Automatic Control, 2023, 35(7):21.DOI:10.1109/TKDE.2022.3186498.

 %\bibitem{engelmann2021conditional}
 %ENGELMANN, LESSMANN, STEFAN. Conditional Wasserstein GAN-based oversampling of tabular data for imbalanced learning[J]. Expert Systems with Applications, 2021, 174: 114582.

 %\bibitem{ho2020denoising}
 %HO, JAIN, ABBEEL et al. Denoising Diffusion Probabilistic Models[C]. Advances in Neural Information Processing Systems, Red Hook, NY, 2020: 6840-6851.

 %\bibitem{hardt2016equality}
 %HARDT, PRICE, SREBRO et al. Equality of Opportunity in Supervised Learning[C]. Advances in Neural Information Processing Systems, Red Hook, NY, USA, 2016: 3315-3323.

 %\bibitem{rudin2019stop}
 %RUDIN, CYNTHIA. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead[J]. Nature Machine Intelligence, 2019, 1(5): 206-215.

 %\bibitem{esteva2017dermatologist}
 %ESTEVA, KUPREL, NOVOA et al. Dermatologist-Level Classification of Skin Cancer with Deep Neural Networks[J]. Nature, 2017, 542(7639): 115-118.

 %\bibitem{miotto2018deep}
 %MIOTTO, WANG, WANG et al. Deep Learning for Healthcare: Review, Opportunities and Challenges[J]. Briefings in Bioinformatics, 2018, 19(6): 1236-1246.

 %\bibitem{lee2015cyber}
 %LEE, BAGHERI, KAO et al. A Cyber-Physical Systems Architecture for Industry 4.0-Based Manufacturing Systems[J]. Manufacturing Letters, 2015, 3(1): 18-23.

 %\bibitem{tao2018digital}
 %TAO, CHENG, QI et al. Digital Twin-Driven Product Design, Manufacturing and Service with Big Data[J]. The International Journal of Advanced Manufacturing Technology, 2018, 94(9): 3563-3576.

 %\bibitem{wang2019deep}
 %WANG, MA, ZHANG et al. Deep Learning for Smart Manufacturing: Methods and Applications[J]. Journal of Manufacturing Systems, 2018, 48: 144-156.

 %\bibitem{zhang2019short}
 %LEE, EO, JUNG et al. Short-term traffic prediction with deep neural networks: A survey[J]. IEEE Access, 2021, 9: 54739-54756.
\end{thebibliography}
\clearpage

