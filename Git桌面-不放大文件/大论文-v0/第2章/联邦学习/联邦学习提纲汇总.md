```
2.3 联邦学习相关理论  
　2.3.1 联邦学习的模型架构  
　2.3.2 联邦学习的训练过程  
　2.3.3 联邦学习的经典算法
　



联邦学习理论
	联 邦 学 习 定 义
	联 邦 学 习 分 类
	
联邦学习
	联邦学习的架构与流程
	联邦学习的分类
	
	

2.1 联邦学习  
　2.1.1 联邦学习简介  
　2.1.2 联邦学习分类
　

1 联邦学习
1.1 联邦学习概念
1.2 联邦学习分类
```



# 2.2 联邦学习

在过去几年中，深度学习取得了迅速进展，尤其在人工智能和智能制造等领域\cite{zhou2016attention}。然而，一些障碍依然存在，包括数据治理方面的挑战以及敏感信息的保护。联邦学习作为一种有前景的方法浮现出来，用以应对这些问题。接下来的部分将对联邦学习进行概述，探讨其概念、架构框架和训练方法，从而加深我们对其演变过程和潜在应用的理解。

## 2.2.1 联邦学习概念

传统机器学习依赖数据集中化处理，但现实中数据以孤岛形式分散在医疗机构、金融机构和企业部门中。例如，医疗领域需大量专业标注数据，但医生标注成本极高（需1万人耗时10年完成有效数据收集），而集中存储可能面临勒索攻击或内部泄露风险（如Facebook数据泄露事件引发社会抗议）。这种模式不仅难以满足跨领域建模需求（如产品推荐需融合销售数据与支付数据），更因违反《通用数据保护条例》（GDPR）等法规面临巨额罚款。数据天然分布于移动终端、医院影像系统、银行风控数据库等场景，医疗影像数据涉及患者隐私，金融交易数据包含商业机密，其敏感性要求物理隔离存储。据统计，全球80%以上有效数据存储于封闭系统，仅医疗行业每年因数据壁垒造成的AI模型误诊损失超50亿美元。这种分散性导致单一机构数据维度残缺（如医院缺乏患者消费特征），严重制约模型泛化能力。GDPR（2018）明确规定数据传输需用户明确授权，中国《网络安全法》要求数据交易需明确保护义务。典型案例如跨国药企因未获患者同意共享临床试验数据被欧盟处罚2.3亿欧元，迫使行业寻求合规解决方案。法规约束使传统数据交易所模式难以为继，催生隐私计算技术需求。头部互联网企业将数据视为核心资产，如电商平台间用户画像数据互不开放，导致推荐系统准确率降低40%。同一集团内部亦存在数据壁垒，某跨国银行信用卡部门与财富管理部门因考核机制差异，拒绝共享客户消费数据，致使反欺诈模型漏报率增加28%。这种"数据封建主义"现象凸显协同建模的迫切需求。2016年谷歌为解决安卓用户输入法模型更新的隐私问题，首次提出联邦学习框架。

联邦学习，有时也被称为协同学习，是一种机器学习方法，其中多个参与方——通常称为客户端——在不将数据集中存储的前提下，共同开发一个模型\cite{kairouz2021advances}。该方法的一个关键特点是数据的内在异质性；由于数据仍然分布在各个客户端，各地点可用的样本往往不符合独立同分布（i.i.d.）的模式。联邦学习主要是由数据隐私、数据最小化和访问权限等问题所驱动。其应用涵盖了多个领域，包括国防、电信、物联网以及制药。

联邦学习旨在开发机器学习算法——例如深度神经网络——以便在不同节点上存储的各个本地数据集上进行训练，而无需直接交换原始数据。相反，每个节点在自身数据上训练本地模型，并在定期的时间间隔内互相共享模型参数（例如权重和偏置）。这种协同的参数交换有助于构建一个惠及所有参与节点的全局模型。

联邦学习与分布式学习的主要区别在于对本地数据集特性所作假设不同\cite{konevcny2015federated}。分布式学习旨在利用并行计算能力，通常假设每个本地数据集均为独立同分布（i.i.d.）且规模大致相同。而联邦学习则不做这些假设，它设计用于处理异构数据，数据集的规模可能存在显著差异。此外，联邦学习中的客户端往往较为不可靠，因其通常依赖于如Wi-Fi等较不稳定的通信方式，并运行在电池供电的设备（如智能手机或物联网设备）上，因此面临更高的失败或中途退出风险。相比之下，分布式学习通常依赖于数据中心内的节点，这些节点具备强大的计算能力，并通过高速网络互联\cite{kairouz2021advances}。



联邦学习的核心思想是在不共享数据的情况下训练模型，仅交换模型参数，其系统架构如图所示。



工作流程如下：

聚合器初始化全局模型，并将模型参数广播至所有客户端
客户端节点接收全局模型参数后，在本地数据上进行训练
客户端将更新后的本地模型（不包含原始数据）发送回聚合器
聚合器对各客户端的模型参数进行加权平均等聚合操作，生成新全局模型
重复上述步骤直至模型收敛或达到最大迭代次数







## 2.2.2 联邦学习分类

McMahan团队在《Communication-Efficient Learning of Deep Networks from Decentralized Data》中构建首个横向联邦学习系统，实现10万移动设备协同训练文本预测模型，数据保留在本地设备，仅上传加密梯度参数。该技术使模型更新延迟降低63%，成为分布式机器学习里程碑。
采用"数据不动、模型动"范式，各参与方通过安全聚合协议交换模型参数。以医疗联合诊断为例：三家医院本地训练CT影像识别模型，协调方使用加权平均算法融合各医院模型参数，形成全局模型后回传更新。整个过程原始数据始终封闭，通过加密传输（如Paillier同态加密）确保中间参数不可逆推原始数据。结合差分隐私与多方安全计算构建双重保障：在梯度更新阶段添加拉普拉斯噪声（ε=0.5），使单条数据贡献隐匿于群体；采用安全多方计算协议实现模型参数的安全聚合，确保参与方无法获知他人参数值。经测试，该方案在MNIST数据集上准确率仅下降1.2%，但成员推理攻击成功率从78%降至3.5%。正式定义为：在满足$V_{FED} \geq V_{SUM} - \delta$约束下（$V_{FED}$为联邦模型效果，$V_{SUM}$为集中式模型效果），多个参与方通过加密参数交互实现联合建模的机器学习框架。根据数据对齐特征可分为三类：  

1. **横向联邦学习**：适用于特征重叠高而样本重叠低的场景（如不同地区医院），采用参数服务器架构，联邦平均（FedAvg）为典型算法  
2. **纵向联邦学习**：针对样本重叠多但特征差异大的场景（如银行与电商），需通过隐私集合求交（PSI）对齐用户ID，采用联邦随机森林等算法  
3. **联邦迁移学习**：解决用户与特征均不对齐问题（如跨语言推荐系统），借助领域适配网络实现知识迁移

在医疗领域支持多医院联合肿瘤筛查（如IBM Watson与Mayo Clinic合作项目使肺癌检出率提升19%）；金融行业用于反洗钱模型训练（微众银行联邦系统连接40家机构，使欺诈识别AUC达到0.92）；智能手机端实现个性化服务（Google Gboard通过联邦学习减少60%云端数据传输量）。据Gartner预测，到2026年联邦学习将覆盖75%的隐私敏感型AI应用场景。

横向联邦学习（HFL）是一种以特征对齐为核心的联邦学习范式，其核心特征在于各参与方的数据特征空间高度重叠，但样本空间差异显著。具体而言，各参与方在数据矩阵的横向划分中共享相同的特征维度（如用户属性、业务指标），但覆盖的用户群体或样本ID存在明显区隔。例如，多家不同地区的银行可能采用相同的风控模型特征（如收入水平、信用评分），但由于地域限制，其客户群体基本无交集。这种形式特别适用于同业态机构间的协作，如跨医院联合训练疾病诊断模型时，各机构虽拥有患者的身高、血压等相同医学特征，但样本来自不同区域的患者群体。横向联邦学习的核心优势在于通过聚合分布式样本提升模型泛化能力，同时避免原始数据流通带来的隐私风险。典型实现方式包括Google提出的移动端联邦框架，通过设备本地训练与云端参数聚合，实现安卓用户输入习惯的全局优化，而无需上传个人聊天记录。

纵向联邦学习（VFL）聚焦于样本空间的高度对齐与特征空间的互补性，适用于跨行业、多模态数据的协同建模场景。其核心特点是参与方共享相同用户ID集合，但各自持有不同的特征维度。例如，某地区银行与电商平台可能共同服务同一批居民，银行掌握用户的金融交易数据，而电商则拥有消费行为特征，通过纵向联邦学习可构建涵盖资金流动与购物偏好的联合信用评估模型。在此过程中，需通过隐私集合求交（PSI）技术实现加密样本对齐，确保各参与方仅暴露共有用户ID，而特征数据始终保留在本地。纵向联邦学习的技术难点在于梯度加密与安全聚合，常用同态加密或安全多方计算（SMPC）实现参数交互，例如医疗场景中多家医院联合训练肿瘤预测模型时，各方仅共享加密后的中间梯度，避免基因数据泄露。此形式显著扩展了特征维度，尤其适用于金融风控、精准营销等需多维度用户画像的场景。

联邦迁移学习（FTL）是针对数据异构性问题的创新解决方案，适用于参与方在样本与特征空间均存在显著差异的场景。当源域与目标域数据分布差异较大时（如中国银行与美国电商用户群体及特征均无重叠），传统联邦学习易出现负迁移现象，而FTL通过知识迁移机制实现跨域协作。例如，在智能医疗领域，利用大型综合医院的预训练模型，通过参数迁移与领域自适应技术，帮助基层医疗机构在有限数据下提升诊断精度。关键技术包括特征空间映射、对抗生成网络（GAN）等，如在自动驾驶中，车企可通过迁移学习将模拟环境训练的模型参数迁移至真实路测设备，同时利用联邦框架保护各厂商的道路数据隐私。该形式的挑战在于平衡隐私保护与知识迁移效率，常结合差分隐私（DP）在梯度更新中注入噪声，防止模型反演攻击。研究表明，联邦迁移学习在冷启动问题与小样本学习场景中表现突出，如在金融领域新产品的信用评估模型中，通过迁移既有产品的风控知识，可将模型收敛速度提升40%以上。



# 参考文献

```
[1] Zhou P, Shi W, Tian J, et al. 2016. Attention-based bidirectional long short-term memory networks for relation classification[C]//Proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: Short papers), 207-212.


[2]Kairouz, Peter; McMahan, H. Brendan; Avent, Brendan; Bellet, Aurélien; Bennis, Mehdi; Bhagoji, Arjun Nitin; Bonawitz, Kallista; Charles, Zachary; Cormode, Graham; Cummings, Rachel; D’Oliveira, Rafael G. L.; Eichner, Hubert; Rouayheb, Salim El; Evans, David; Gardner, Josh (2021-06-22). "Advances and Open Problems in Federated Learning." Foundations and Trends in Machine Learning. 14 (1–2): 1–210. arXiv:1912.04977. doi:10.1561/2200000083. ISSN 1935-8237.

[3]Konečný, Jakub; McMahan, Brendan; Ramage, Daniel (2015). “Federated Optimization: Distributed Optimization Beyond the Datacenter”. arXiv:1511.03575 [cs.LG].

[4]Kairouz, Peter; Brendan McMahan, H.; Avent, Brendan; Bellet, Aurélien; Bennis, Mehdi; Arjun Nitin Bhagoji; Bonawitz, Keith; Charles, Zachary; Cormode, Graham; Cummings, Rachel; D’Oliveira, Rafael G. L.; Salim El Rouayheb; Evans, David; Gardner, Josh; Garrett, Zachary; Gascón, Adrià; Ghazi, Badih; Gibbons, Phillip B.; Gruteser, Marco; Harchaoui, Zaid; He, Chaoyang; He, Lie; Huo, Zhouyuan; Hutchinson, Ben; Hsu, Justin; Jaggi, Martin; Javidi, Tara; Joshi, Gauri; Khodak, Mikhail; et al. (10 December 2019). “Advances and Open Problems in Federated Learning”. arXiv:1912.04977 [cs.LG].
```

```
@inproceedings{zhou2016attention,
  title={Attention-based bidirectional long short-term memory networks for relation classification},
  author={Zhou, Peng and Shi, Wei and Tian, Jun and Qi, Zhenyu and Li, Bingchen and Hao, Hongwei and Xu, Bo},
  booktitle={Proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: Short papers)},
  pages={207--212},
  year={2016}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}
@article{konevcny2015federated,
  title={Federated optimization: Distributed optimization beyond the datacenter},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, Brendan and Ramage, Daniel},
  journal={arXiv preprint arXiv:1511.03575},
  year={2015}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

```

````
```latex
\section{联邦学习的主要形式与分类}
\label{sec:fl_types}

联邦学习可根据特征空间和样本空间的重叠情况，主要划分为横向联邦学习（Horizontal Federated Learning）、纵向联邦学习（Vertical Federated Learning）以及联邦迁移学习（Federated Transfer Learning）\cite{yang2019federated,li2020federated}。这种划分方式能够帮助研究人员与工程实践者在不同场景下选择适宜的联邦学习方案，从而更高效地利用分布式数据进行模型训练。

在横向联邦学习（HFL）中，各数据拥有方在特征空间上较为相似，但用户或样本并不重叠\cite{yang2019federated,liu2019communication}。举例来说，若多家银行都想联合训练一个信用风险评估模型，但它们各自的用户群体并无明显重叠，且每家银行都有较为类似的特征字段（如年龄、职业、收入等），这种场景便适合采用横向联邦学习。通过让各银行保留本地数据，仅传递模型参数或梯度进行聚合，既能提升模型的泛化能力，又在最大程度上保护了用户隐私。

与之相对，纵向联邦学习（VFL）则适用于样本空间高度重叠，但特征集差异较大的场景\cite{liu2020secure,chen2020vafl}。例如，某些银行与电商平台在用户群体上可能有很大交集，却掌握了不同维度的用户信息：银行侧有用户财务信用数据，电商侧则有用户消费行为数据。在这种情形下，联合建模可以充分利用双方不同来源的特征信息，从而获得更全面、更准确的刻画。当各方在训练过程中只需对对齐后的公共用户进行联合梯度更新时，无需共享原始数据，依然能够保证个人隐私与数据安全。

第三种形式是联邦迁移学习（FTL），其特点在于样本空间与特征空间均不完全重叠\cite{yang2019federated,chen2020vafl}。在部分数据量较少或者特征不足的场景里，可以通过迁移学习的方法来补充数据或特征不足的问题。此时，不同组织或机构拥有的用户群体与特征集合可能仅存在少量重叠，但依然可通过联邦迁移学习的思想，将部分预训练模型的知识进行迁移，从而提升任务的性能或模型的泛化能力。这种方法特别适用于小样本或垂域知识不足的情景，有效地拓展了联邦学习在更多应用场景下的可行性。

综上所述，横向联邦学习、纵向联邦学习及联邦迁移学习分别应对了特征空间或样本空间上的不同重叠情况。它们在数据类型、业务场景和技术实现上各有侧重，却都遵循了联邦学习的核心理念：在不直接交换数据的前提下，实现多方协作与知识共享。这种多元化的联邦学习形式为不同行业和应用需求提供了灵活的选择，也在隐私保护和跨域协作方面展现出巨大的潜力。

\begin{spacing}{1.0}
\bibliographystyle{IEEEtran}
\begin{thebibliography}{00}



\end{thebibliography}
\end{spacing}
```
````

```
\bicaption[\xiaosi 联邦学习分类]{\wuhao 联邦学习分类。(a) 横向联邦学习；(b) 纵向联邦学习；(c) 迁移联邦学习}{\wuhao Federated Learning Classification. (a) Horizontal Federated Learning; (b) Vertical Federated Learning; (c) Transfer Federated Learning}
```

