```
### 2.2 半监督学习基础
1. 半监督学习定义  
   - 介于有监督学习和无监督学习之间  
   - 充分利用少量标注数据和大量未标注数据  
2. 常见的半监督学习方法  
   - 自训练、共训练、伪标签、图半监督等  
   - 半监督学习在推荐系统、医疗诊断等领域的典型应用与挑战  
3. PU（Positive and Unlabeled）学习  
   - 仅有正样本与未标注样本的特殊半监督场景  
   - 关键难点：未标注数据中既包含真正的负样本也可能包含正样本  
   - 常见方法：两步技术、PU Bagging 等  
```

```
以下是一份较为详细的「半监督学习」章节提纲示例，可在此基础上根据论文需要进行增删和调整。

---

## 1　半监督学习（Semi-Supervised Learning）
### 1.1　半监督学习定义
第一自然段: 背景与动机。第一句话：有标注数据稀缺、标注成本高昂；第二句话：充分利用大量未标注数据提升模型性能  
第二自然段: 核心思想。第二句话：同时利用少量标注数据与大量未标注数据进行训练；第二句话：通过对未标注数据的“挖掘”或“自学习”，弥补标注样本不足  
第三自然段: 半监督学习的基本假设。第一句话：平滑性假设（Smoothness Assumption）：在特征空间距离较近的样本，其标签也应相近；第二句话：聚类假设（Cluster Assumption）：同一簇中的样本倾向于具有相同的标签；第三句话：低密度分隔假设（Low-density Separation Assumption）：不同类别被低密度区域分隔  
第四自然段: 与监督学习、无监督学习的联系与区别。第一句话：与监督学习相比，半监督学习更强调对未标注数据的利用；第二句话：与无监督学习相比，半监督学习依然利用少量标注信息进行指导  

### 1.2　常见的半监督学习方法

第一自然段: 基于一致性正则化的方法（Consistency Regularization）  
   - 思路：对同一无标注样本在不同扰动下模型输出应保持一致  
   - 代表算法：\(\Pi\)-model、Mean Teacher、VAT（虚拟对抗训练）等  
第二自然段:  基于伪标签（Pseudo-Labeling）的方法  
   - 思路：对无标注样本进行预测，挑选置信度高的结果作为伪标签  
   - 优点与局限：过程简单；但错误的伪标签会被放大  
第三自然段:  图（Graph）方法  
   - 思路：使用图结构将相似样本连接起来，进行标签传播或图正则化  
   - 代表算法：Label Propagation、Label Spreading、Graph Convolutional Networks (GCNs) 相关方法  
第四自然段:  生成式模型（Generative Methods）  
   - 思路：利用生成模型（如变分自编码器、GAN）同时学习数据分布与标签信息  
   - 代表算法：Semi-Supervised GAN、VAE + Classifier  
第五自然段:  自训练（Self-Training）或协同训练（Co-Training）  
   - 思路：使用已有分类器对未标注数据进行预测，将高置信度样本加入标注集，反复迭代  
   - 协同训练：多视图学习中常用，两套分类器相互标注无标注数据  
第六自然段:混合方法与其他前沿方向  
   - 混合使用一致性正则化、伪标签、对抗训练等多种技术  
   - 在深度学习场景下的发展与新趋势  

### 1.3　PU（Positive and Unlabeled）学习

1. PU 学习的定义与背景  
   - 仅拥有正样本（Positive）和未标注样本（Unlabeled），无负样本明确标注  
   - 典型应用：推荐系统中“点击/未点击”数据、医学诊断中“阳性/未知”数据等  
2. PU 学习的主要思路  
   - 基于样本选择：从未标注集中“筛选”出可疑负样本或部分正样本进行迭代训练  
   - 基于重加权或损失修正：对未标注样本的损失赋予不同权重或采用统计方法区分正负样本分布  
3. 典型算法与策略  
   - Two-step methods：先通过启发式规则或估计方法区分一部分未标注样本为负样本，再训练监督模型  
   - Spy technique / PEBL：利用已有的正样本特征“探测”未标注数据中的潜在负样本  
   - 基于正、负统计分布的学习（nnPU、uPU 等）：将未标注样本视为“混合分布”，对其进行统计推断  
4. 理论研究  
   - PU 学习的一致性与泛化误差边界  
   - 与传统二分类监督学习在算法复杂度、精度等方面的对比  
5. 应用与实践  
   - 互联网推荐/广告：正例常为点击（或购买）用户  
   - 生物医学：已知阳性样本和海量未知样本  
   - 信息检索：用户标记的正例文档与海量未评估文档  
6. 挑战与未来发展  
   - 如何有效区分伪负样本与真正负样本  
   - 大规模数据下的高效算法设计  
   - 噪声标注与对抗样本的影响  

---

以上提纲可作为「半监督学习」及其分支 PU 学习相关内容的写作框架。您可以根据论文的研究重点、篇幅要求以及与自身课题的结合点来对上述提纲进行增减、扩展或强化。
```

我在正在撰写研究生学位论文。需要对方法中使用到的基础理论介绍，我写好了一部分提纲，帮我再细化提纲:

# 1 半监督学习

# 1.1 半监督学习定义

 有标注数据通常十分稀缺，且标注成本高昂，主要原因在于人工标注往往需要大量的人力、时间和专业知识投入。同时，随着数据采集技术的不断进步，各领域能够收集到的数据量呈指数级增长，未标注的数据在大多数情况下都要远多于已标注的数据。如何最大化地利用这些未标注数据成为了众多研究者关注的焦点。一方面，标注数据不足会导致模型难以充分学习到样本的多样性特征，导致模型的泛化能力受到限制；另一方面，若能够将海量的未标注数据纳入模型的训练过程，则有望在较少的人工标注投入下，显著提升模型的学习效果和最终的性能表现。因此，半监督学习应运而生，其核心目标便是通过对海量未标注数据的有效利用，弥补标注数据不足的问题，以期在降低标注成本的同时获得与全监督模型相近甚至更优的性能。
 半监督学习的核心思想在于：同时利用少量标注数据与大量未标注数据进行训练，从而让模型能够在学习过程中充分“感知”到数据分布的结构特征。一方面，少量的标注数据为模型提供了基本的监督信息，即哪些特征与目标标签之间存在相关性；另一方面，庞大的未标注数据则可被视为对潜在真实数据分布的重要补充，通过对未标注数据进行“挖掘”或“自学习”，模型能够在特征空间中更好地构建关于不同类别或标签的决策边界。这种做法的关键在于，未标注数据本身虽然缺乏显式的标签信息，但它们在特征空间所呈现出的分布规律却可以帮助模型更好地理解数据整体的结构，从而弥补标注样本不足可能带来的泛化性能不足、过拟合风险增大的问题。综合而言，半监督学习不仅进一步提高了模型的鲁棒性和预测精度，也有效减少了对人工标注的依赖，具有重要的研究价值和应用潜力。

半监督学习要想在充分利用未标注数据的同时，避免引入过多噪声或错误信息，需要在一定的理论假设下运作。首先，平滑性假设（Smoothness Assumption）指出：在特征空间距离较近的样本，其标签也应相近。也就是说，如果两个数据点在输入特征空间非常接近或相似，那么它们被划分到相同或相似类别的可能性更高。这个假设在监督学习和无监督学习中都时常被使用，是构建大多数学习算法的基础。其次，聚类假设（Cluster Assumption）认为：同一簇中的样本往往倾向于具有相同的标签。如果在高维特征空间里，可以清晰地将样本分成若干簇，那么同簇的样本通常共享类似的语义或特征，也更有可能共享相同的标签。最后，低密度分隔假设（Low-density Separation Assumption）强调：不同类别的数据点往往通过低密度区域进行分隔。也就是说，在特征空间中，可以通过寻找稀疏区域来划分不同的类别边界，从而将异类样本分割开。这些假设为半监督学习算法提供了理论指导，使得算法在面对海量未标注数据时能够更稳健地推断出潜在的类别信息。

与监督学习相比，半监督学习更强调对未标注数据的利用。在传统的监督学习中，模型只依赖带有人工标签的数据进行训练，未标注的数据很少被直接纳入学习过程；而在半监督学习中，未标注数据可以提供丰富的分布信息，帮助模型更全面地理解数据特征。对比无监督学习，半监督学习依然利用少量标注信息进行指导，因此在训练过程中能够通过有限的标签信息来校正模型，使其更符合真实任务需求，减少纯粹的无监督学习可能出现的标签错配或簇划分结果与实际目标不符的情况。换言之，半监督学习既不像监督学习那样完全依赖标注数据，也不像无监督学习那样完全不需要标注，而是综合两者优点，既在一定程度上依赖标注信息确保学习方向的准确性，又充分利用海量的未标注数据来提升模型对数据分布的刻画能力与泛化能力。也正因为这种“折中与融合”的特性，半监督学习在实际应用场景中展现出了独特的优势，对学界和工业界都具有重要意义。

# 1.2 半监督学习方法

# 1.2.1 协同训练算法

协同训练（Co-training）是一种基于分歧的多视角学习框架，其核心思想在于通过不同特征子集构建互补视角，在少量标注数据与大量未标注数据的协同作用下提升分类性能。该方法假设数据存在两个充分冗余且条件独立的视图，每个视图都能独立支持分类任务，而两个分类器之间则通过交叉验证实现知识迁移。例如，在医学图像分割任务中，可利用对抗样本生成技术构建互补视图，从而为后续协同训练奠定基础。

在初始化阶段，方法首先利用有限的标注数据分别训练两个分类器，如支持向量机或随机森林等，每个分类器均基于独立视图的特征子集进行学习。接下来，训练好的分类器对大量未标注数据进行预测，并通过置信度评估方法（如概率阈值、熵值、不确定性感知方法或通过蒙特卡罗采样估计预测不确定性）来筛选出高置信度的样本。此过程中，贝叶斯分类器可能将后验概率转化为置信度，而支持向量机则可以通过间隔大小来衡量，进而动态调整损失权重，以确保选择出的数据具有较高的可靠性。

筛选出的高置信度伪标签样本随后被加入到另一分类器的训练集中，形成一种增量学习循环，这一过程可通过对抗性训练（例如利用FGSM生成对抗样本）来进一步增强视图间的多样性并提升模型的鲁棒性。整个训练过程通过不断重复预测、筛选、增强与再训练这几个步骤，直至满足预设的终止条件，如迭代次数限制、分类器稳定性或性能收敛性等要求。研究表明，在视图满足充分冗余性条件下，即便初始存在弱分类器，经过多次迭代，模型的整体分类精度仍可显著提高。

协同训练方法的理论优势在于显著降低了对标注数据的依赖，使其在文本分类、图像分割等实际应用中展现出优越性能，并且在部分场景中推理速度较基线模型可提升4.8倍。然而，这种方法也面临诸多挑战：首先，构建完全条件独立的视图在实际应用中较为困难，例如网页内容与链接信息往往存在高度相关性；其次，置信度评估方法需要与具体分类器相适配，否则容易引入噪声；最后，多视图生成技术（如知识蒸馏和对抗样本生成）对算法设计提出了更高要求，增加了实际应用的难度。

为了突破传统协同训练方法的局限性，研究者提出了多种改进方向。例如，COREG方法通过采用不同距离度量的KNN分类器，并借助邻近属性耦合来增强置信度估计；Tri-training方法则引入第三个分类器，只有在多数分类器达成一致时才对样本进行标记，从而有效降低噪声的影响；此外，深度协同训练结合了对抗训练与跨视图一致性损失，有效提升了特征表达能力。总体来看，尽管协同训练在视图划分有效性和分类器设计科学性等核心问题上仍有待进一步探索，但其在轻量化模型微调（如DISCO框架）和医学影像分析等领域已展现出重要应用潜力。

## 1.2.2 伪标签生成半监督学习方法

该方法基于自训练假设，通过为未标记数据生成伪标签来实现半监督学习，并将这些伪标签作为额外的训练数据。未标记数据对应的损失项通常可以表示为

$$
L_u = \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{K} R \left( y_i^{j}, p(x_i | \theta) \right) \tag{2-1}
$$

其中，$ p(x_i | \theta) = \text{softmax}(f(\theta, x_i)) $ 为模型 $ f(\theta) $ 经过 softmax 后获得的类别概率分布，而 $ y_i^{j} $ 则指生成的伪标签，其形式可以是一热编码或概率向量。本文采用一热编码，以实现熵最小化的目的。熵最小化有助于加强低密度区域分隔的假设，即通过使模型在预测时更加自信，将决策边界尽量安排在数据稀疏的区域。为了在使用软标签的情况下达到熵最小化，通常会对伪标签进行锐化（例如采用0-1阈值化），其过程可简单描述为 $ y_i^{j} = \text{argmax}(p(\theta', x_i)) $，即直接以模型的预测作为伪标签。

在上述表达式中，$ R(\cdot, \cdot) $ 表示用于度量差异的距离函数，其常见选项包括均方误差（MSE）和交叉熵等。为了进一步保留那些置信度较高的伪标签样本，对公式 (2-1) 进行了更深层次的优化，通过引入一个筛选函数 $ \text{Filter}(\cdot) $ 来实现：

$$
L_u = \frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{K} \text{Filter} \left( R \left( y_i^{j}, p(x_i | \theta) \right) \right) \tag{2-2}
$$

此外，为了构建总的目标函数，通常会将带标签数据上的监督损失与未标记数据上的伪标签损失进行结合，从而得到如下形式：

$$
L = L_s + \alpha L_u \tag{2-3}
$$

其中，$ \alpha $ 用于平衡两部分损失，其取值既可以是预先设定的超参数，也可以是一个随训练迭代而动态调整的函数（例如 $ \alpha = \alpha(t) $）。通过计算 $ L $ 的梯度并更新模型参数，模型便能够在每次迭代中重新预测未标记数据的伪标签，不断优化直至收敛或满足预定的停止准则，从而完成半监督学习的循环过程。



## 1.2.3 基于图的半监督学习方法

基于图的策略在半监督学习中发挥着关键作用，通过精心构建捕捉数据点关系的图结构显著提升了学习效率。该方法将每个样本转化为图节点，并根据相似性或距离进行连接，通过边权编码相似程度。利用这种图结构，信息可以从已标注样本传递至未标注样本，这对于揭示数据内在模式特别有利——尤其在数据自然形成簇结构的场景中。

然而，构建和维护大规模图需要大量计算和存储资源，且图构建策略和标签传播方式会直接影响模型效果。尽管存在这些挑战，基于图的方法通过高效利用未标注数据并增强可解释性，在半监督学习领域确立了独特地位。

监督学习中的一个基本假设是相似样本共享相同类别标签，这意味着在特征空间中的实例间类别标签应平滑变化。基于这种平滑性假设，k近邻分类器等方法通过邻近标注样本推断未标注数据的标签。但当标注数据稀缺或被严重噪声干扰时，监督方法可能失效。半监督学习通过假设更全局的平滑性或聚类特性（即聚集的数据应属同类）来应对这一问题。图基半监督学习（GSSL）利用这一原理，通过在图中连接相似节点来捕获整体数据结构，因此密集连接的节点区域倾向于共享相同类别标签。GSSL为每个节点$i$和类别$c$分配平滑传播于图中且符合现有标签信息的分数$F_{ic}$，其数学形式化为：

$$
\min_F \frac{1}{2} \text{tr}(F^T L F) + \lambda \| F - B \|^2 
\quad \text{约束条件} \quad F1 = 1
$$

式中$L$表示相关线性算子或邻接矩阵，$F$为标签分数构成的$n$维向量，$B$为$n$维标签特征矩阵（对于未标注节点$i$有$B_{ic}=1$，对已标注为类别$c$的节点$i$有$B_{ic}=0$）。公式(4-1)第一项强制图上的平滑性，第二项使$F$与初始标签矩阵$B$对齐，系数$\lambda$控制二者的平衡。

不同的$L$和$\lambda$设置对应特定GSSL算法。例如当$\lambda=0$且定义$L = D - A$（其中$D$为度矩阵，$D_{ii}=\sum_j A_{ij}$）时对应谱波函数方法。更相关的是采用$L = D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$和$\lambda=\bigl(\tfrac{1}{\alpha}-1\bigr)$，代入公式(4-1)得到局部全局一致性目标：

$$
E(F) = \frac{1}{2} \sum_{i,j} W_{ij} \sum_c
\left\|\frac{F_{ic}}{\sqrt{D_{ii}}} - \frac{F_{jc}}{\sqrt{D_{jj}}}\right\|^2
+ \left(\frac{1}{\alpha} - 1\right) \sum_i \sum_c \|F_{ic} - B_{ic}\|^2
$$

尽管公式(4-2)存在闭式解，但常需矩阵求逆运算，这在计算和存储上代价高昂。考虑到$G$通常构造为稀疏矩阵（通过加权相似函数或k近邻连接），此类运算可能不可行。更实用的方法是采用迭代幂法更新$F$：

$$
F_{t+1} = Z^{-1} \bigl((1 - \alpha) B + \alpha L F_t\bigr)
$$

式中$Z^{-1}$为对$F$行归一化的对角矩阵。该式常被称为标签传播，通过参数$\alpha$（$0 \le \alpha \le 1$）控制标签在网络中的渗透速度。收敛后，节点$i$的类别由最大分数标签确定：

$$
y_i^ = \arg\max_c F_{ic}
$$

## 1.2.3 PU（Positive and Unlabeled）学习

正例与未标注（PU）学习是一种半监督方法，仅依赖于少量已确认的正例和大量未标注数据进行训练，完全不需要负例 

在传统二分类任务中，通常使用包含平衡正负例的完全标注数据集来训练模型。然而，在当今数据爆炸的时代，人工标注成本不断攀升，高质量标注的获取变得愈发困难，导致大量真实世界的数据缺乏标注。PU 学习能够利用这些未标注数据与少量正例直接学习决策边界，从而削弱对全面标注的依赖。

该方法通过以下多种策略提升分类效果：通过近似未标注数据中潜在负例的分布来纠正偏差，从而减弱由类别不平衡带来的偏差。降低了对大规模标注的需求，因为只需少量正例即可开始训练。适合用于工业控制等系统的异常检测，此类系统通常异常事件极为稀少，意味着未标注数据大多代表正常状态，从而使该方法能有效捕捉微弱的负例特征并增强分类器的鲁棒性。

以下是用户提供的论文段落的中文翻译，结合相关文献进行了术语统一与解释优化：

直接应用标准分类器 

一种简单直接的方法是将已标记的正例样本视为正类，未标记样本视为负类进行训练。尽管方法简单，但研究表明其仍能产生有效结果。Elkan与Noto（2008）证明，在满足特定假设条件下（如正例样本完全随机标记），仅通过正例和未标记数据训练的分类器，其输出的分数与使用完整标注数据（正负类均明确）训练的分类器具有正比关系。因此，若只需对样本属于正类的可能性进行排序，这种简化方法已足够有效——其排序能力几乎等同于基于完整标注数据训练的模型。

PU数据下的袋装法（Bagging）

Mordelet与Vert（2013）提出了一种改进的袋装法（PU Bagging），具体流程如下：  
1. **自助采样法**：构建训练集时，包含所有正例样本，并从未标记池中通过有放回随机抽样选取子样本。  
2. **分类器训练**：基于新生成的“正例+伪负例”自助样本训练分类器。  
3. **袋外评估**：将训练好的模型应用于未参与当前自助抽样的未标记样本（即袋外样本），记录其预测分数。  
4. **迭代与聚合**：重复多次上述步骤，每个未标记样本将累积多个袋外分数，最终分数取平均值。  

研究表明，该方法在正例样本稀缺或未标记数据中负类比例较低时表现优异，甚至超越其他先进PU学习方法，且在处理大规模未标记数据时具有更高的计算效率。

两步法策略

多数PU学习算法可归为“两步法”，其核心流程为：  
1. **可靠负样本识别**：从未标记数据中筛选出高置信度的负类子集（可能规模较小但质量高），作为后续训练的基准。  
2. **迭代分类器训练**：结合已知正例与可靠负样本训练初始分类器，将其应用于剩余未标记数据。典型实现中，此过程会迭代进行：新识别的负样本被加入可靠负集，分类器随之更新，直至达到收敛或预设停止条件。  

尽管Shubham Jain在半监督学习中的伪标签思想与此类似，但两步法专为PU场景设计，通过动态调整可靠负集逐步提升分类性能。  

