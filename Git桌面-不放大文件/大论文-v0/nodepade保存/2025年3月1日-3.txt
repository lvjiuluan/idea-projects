以下是优化后的版本，使其更加符合学术论文的严谨性、科学性和可读性：

---

### 问题设置

本文研究的纵向联邦学习（Vertical Federated Learning, VFL）场景涉及两个参与方（Party A 和 Party B），其中仅有一方拥有标签信息。该问题设置参考了文献 [40] 中定义的典型 VFL 结构。

#### 数据集定义

- Party A 拥有数据集：
  $$
  \mathcal{D}^A := \{X^A_i\}_{i=1}^{n^A}
  $$
  其中，$X^A_i$ 表示第 $i$ 个样本的特征向量，$n^A$ 为样本数量。Party A 的数据仅包含特征，不包含标签。

- Party B 拥有数据集：
  $$
  \mathcal{D}^B := \{(X^B_i, Y^B_i)\}_{i=1}^{n^B}
  $$
  其中，$X^B_i$ 为第 $i$ 个样本的特征向量，$Y^B_i \in \{0,1\}^C$ 为对应的独热编码（one-hot encoding）标签，$C$ 为类别数，$n^B$ 为样本数量。Party B 拥有标签信息，但其特征可能不足以单独训练高性能模型，因此需要结合 Party A 的补充特征。

由于 VFL 需要在数据隐私保护的前提下进行联合建模，Party A 和 Party B 不能直接共享各自的数据。因此，双方需通过隐私保护的加密实体匹配技术 [30] 进行样本对齐，得到对齐样本集：

$$
\mathcal{D}_{al} := \{X^A_{i_{al}}, X^B_{i_{al}}, Y^B_{i_{al}}\}_{i=1}^{n_{al}}
$$

其中，$n_{al}$ 为对齐样本的数量。各方分别持有对齐样本的部分数据：

- Party A 持有：
  $$
  \mathcal{D}^A_{al} := \{X^A_{i_{al}}\}_{i=1}^{n_{al}}
  $$
- Party B 持有：
  $$
  \mathcal{D}^B_{al} := \{X^B_{i_{al}}, Y^B_{i_{al}}\}_{i=1}^{n_{al}}
  $$

如果将 $\mathcal{D}^A$ 和 $\mathcal{D}^B$ 连接，并对相同身份的样本进行对齐，则可形成一个完整的数据集（如图 1.b 所示）。然而，由于数据的纵向分割特性，每个参与方仅持有该数据集的一个垂直分区，这正是“纵向联邦学习”名称的由来。

#### 非对齐样本问题

在实际应用中，除了对齐样本外，每个参与方还可能拥有大量非对齐样本，即在另一方数据集中找不到对应匹配的样本：

- Party A 的非对齐样本：
  $$
  \mathcal{D}^A_{nl} := \{X^A_{i_{nl}}\}_{i=1}^{n^A_{nl}}
  $$
- Party B 的非对齐样本：
  $$
  \mathcal{D}^B_{nl} := \{X^B_{i_{nl}}, Y^B_{i_{nl}}\}_{i=1}^{n^B_{nl}}
  $$

从单一表格数据集（图 1.b）的角度来看，每个参与方对于另一方的非对齐样本都缺失相应的特征（或标签）。传统的 VFL 方法通常仅使用对齐样本 $\mathcal{D}_{al}$ 进行模型训练，而忽略非对齐样本 $\mathcal{D}^A_{nl}$ 和 $\mathcal{D}^B_{nl}$。然而，在对齐样本数量有限的情况下，这种做法可能会限制模型的性能，因为大量潜在有用的数据未被充分利用。

---



---

这样��化后的版本更加符合学术论文的风格，逻辑清晰，表达严谨，并且更容易让读者理解你的研究贡献。希望对你有所帮助！ 😊